{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9df9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e7a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)  # This loads the list of combined strings\n",
    "\n",
    "    # Split each entry into [submission, comments] by the separator\n",
    "    processed = []\n",
    "    for entry in data:\n",
    "        if \" [======>] \" in entry:\n",
    "            submission, comments = entry.split(\" [======>] \", 1)\n",
    "            processed.append([submission.strip(), comments.strip()])\n",
    "        else:\n",
    "            # Handle any malformed entry without the separator\n",
    "            processed.append([entry.strip(), \"\"])\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083d49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()  # convert text to lower-case\n",
    "    text = re.sub('&gt;', '', text) # remove some special characters from the data &gt; corresponds to >\n",
    "    text = re.sub('&amp;', '', text) # remove some special characters from the data &amp; corresponds to &\n",
    "    text = re.sub(r'\\s+', ' ', text)  # eliminate duplicate whitespaces using regex\n",
    "    text = re.sub(r'\\[[^]]*\\]', '', text)  # remove text in square brackets\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # remove punctuation (keep only characters and numbers)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7619f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nta_yta(strings):\n",
    "    results = []\n",
    "    for text in strings:\n",
    "        nta_count = len(re.findall(r'\\bnta\\b', text))\n",
    "        yta_count = len(re.findall(r'\\byta\\b', text))\n",
    "        results.append((nta_count, yta_count))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd360d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_json(\"./data/output.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c34d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_submissions = [x[0] for x in corpus]\n",
    "corpus_comment = [x[1] for x in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a8e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_submissions = corpus_submissions[0:5000]\n",
    "corpus_comment = corpus_comment[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9801cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run our function to preprocess all comments\n",
    "preprocessed_comments = [preprocess(comment) for comment in corpus_comment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4579d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_yta = count_nta_yta(preprocessed_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed80b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions_ratios = [p[0] / sum(p) if sum(p) != 0 else 0 for p in nta_yta]\n",
    "plt.hist(proportions_ratios)\n",
    "plt.title(\"nta proportion\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e04a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize the comments and remove the stopwords\n",
    "all_words_nta = [] # list of lists holding our dataset (each list corresponds to a comment and it includes the tokenized words)\n",
    "for comment in preprocessed_comments:\n",
    "    # tokenize the comments and remove stopwords\n",
    "    all_words_nta.append([ w for w in word_tokenize(comment) if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd3feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize the comments and remove the stopwords that have nta proportion > 0.6\n",
    "all_words_nta = [] # list of lists holding our dataset (each list corresponds to a comment and it includes the tokenized words)\n",
    "for comment in range(len(preprocessed_comments)):\n",
    "    if proportions_ratios[comment] >= 0.6:\n",
    "        # tokenize the comments and remove stopwords\n",
    "        all_words_nta.append([ w for w in word_tokenize(preprocessed_comments[comment]) if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate all ngrams for n=2, 3, and 4\n",
    "all_bigrams = []\n",
    "all_trigrams = []\n",
    "all_fourgrams = []\n",
    "\n",
    "# for each document\n",
    "for doc in all_words_nta:\n",
    "    # calculate all ngrams with size 2 (i.e., bigrams) and then store them in our list holding all bigrams\n",
    "    all_bigrams.extend(list(ngrams(doc, 2)))\n",
    "    all_trigrams.extend(list(ngrams(doc, 3)))\n",
    "    all_fourgrams.extend(list(ngrams(doc, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7253ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the Counter class from Collections to find the top N most occurring Ngrams in our dataset\n",
    "top_bigrams = Counter(all_bigrams).most_common(10)\n",
    "top_trigrams = Counter(all_trigrams).most_common(10)\n",
    "top_fourgrams = Counter(all_fourgrams).most_common(10)\n",
    "\n",
    "# lets present the most occurring Ngrams in a nice table using Pandas\n",
    "top_bigrams_df = pd.DataFrame(top_bigrams, columns =['Bigram', '#']) # create DataFrame for bigrams\n",
    "top_trigrams_df = pd.DataFrame(top_trigrams, columns=['Trigram', '#']) # create DataFrame for trigrams\n",
    "top_fourgrams_df = pd.DataFrame(top_fourgrams, columns=['Fourgram', '#']) # create DataFrame for fourgrams\n",
    "ngrams_df = pd.concat([top_bigrams_df, top_trigrams_df, top_fourgrams_df], axis=1) # concatenate all to a single datafrme\n",
    "\n",
    "print(\"The top 10 most popular ngrams in our dataset are...\")\n",
    "ngrams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b7f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ti3160tu-nlp-group-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
