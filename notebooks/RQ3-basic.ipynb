{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9df9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e7a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)  # This loads the list of combined strings\n",
    "\n",
    "    # Split each entry into [submission, comments] by the separator\n",
    "    processed = []\n",
    "    for entry in data:\n",
    "        if \" [======>] \" in entry:\n",
    "            submission, comments = entry.split(\" [======>] \", 1)\n",
    "            processed.append([submission.strip(), comments.strip()])\n",
    "        else:\n",
    "            # Handle any malformed entry without the separator\n",
    "            processed.append([entry.strip(), \"\"])\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083d49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()  # convert text to lower-case\n",
    "    text = re.sub('&gt;', '', text) # remove some special characters from the data &gt; corresponds to >\n",
    "    text = re.sub('&amp;', '', text) # remove some special characters from the data &amp; corresponds to &\n",
    "    text = re.sub(r'\\s+', ' ', text)  # eliminate duplicate whitespaces using regex\n",
    "    text = re.sub(r'\\[[^]]*\\]', '', text)  # remove text in square brackets\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # remove punctuation (keep only characters and numbers)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7619f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nta_yta(strings):\n",
    "    results = []\n",
    "    for text in strings:\n",
    "        nta_count = len(re.findall(r'\\bnta\\b', text))\n",
    "        yta_count = len(re.findall(r'\\byta\\b', text))\n",
    "        results.append((nta_count, yta_count))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(word.lower()) for word in tokens if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd360d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_json(\"./data/output.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c34d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_submissions = [x[0] for x in corpus]\n",
    "corpus_comment = [x[1] for x in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a8e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_submissions = corpus_submissions[0:15]\n",
    "corpus_comment = corpus_comment[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9801cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run our function to preprocess all comments\n",
    "preprocessed_comments = [preprocess(comment) for comment in corpus_comment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4579d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_yta = count_nta_yta(preprocessed_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed80b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions_ratios = [p[0] / sum(p) if sum(p) != 0 else 0 for p in nta_yta]\n",
    "plt.hist(proportions_ratios)\n",
    "plt.title(\"NTA proportion 500 submissions\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e04a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d3027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the comments and remove the stopwords that have yta proportion < 0.5\n",
    "all_words_yta = [] # list of lists holding our dataset (each list corresponds to a comment and it includes the tokenized words)\n",
    "for comment in range(len(preprocessed_comments)):\n",
    "    if proportions_ratios[comment] >= 0.5:\n",
    "        # tokenize the comments and remove stopwords\n",
    "        all_words_yta.append([ w for w in word_tokenize(preprocessed_comments[comment]) if w not in stop_words])\n",
    "\n",
    "        # all_words_yta.append([ w for w in word_tokenize(preprocessed_comments[comment]) if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the comments and remove the stopwords that have nta proportion > 0.5\n",
    "all_words_nta = []\n",
    "for comment in range(len(preprocessed_comments)):\n",
    "    if proportions_ratios[comment] < 0.5:\n",
    "        all_words_nta.append([ w for w in word_tokenize(preprocessed_comments[comment])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0176bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_yta = [lemmatize_tokens(tokens) for tokens in all_words_yta]\n",
    "lemmatized_nta = [lemmatize_tokens(tokens) for tokens in all_words_nta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bigrams_yta = []\n",
    "all_trigrams_yta = []\n",
    "all_fourgrams_yta = []\n",
    "\n",
    "all_bigrams_nta = []\n",
    "all_trigrams_nta = []\n",
    "all_fourgrams_nta = []\n",
    "\n",
    "for doc in lemmatized_nta:\n",
    "    all_bigrams_nta.extend(list(ngrams(doc, 2)))\n",
    "    all_trigrams_nta.extend(list(ngrams(doc, 3)))\n",
    "    all_fourgrams_nta.extend(list(ngrams(doc, 4)))\n",
    "\n",
    "for doc in lemmatized_yta:\n",
    "    all_bigrams_yta.extend(list(ngrams(doc, 2)))\n",
    "    all_trigrams_yta.extend(list(ngrams(doc, 3)))\n",
    "    all_fourgrams_yta.extend(list(ngrams(doc, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bigrams_yta_set = list(set(all_bigrams_yta))\n",
    "all_trigrams_yta_set = list(set(all_trigrams_yta))\n",
    "all_fourgrams_yta_set = list(set(all_fourgrams_yta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4969b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter al the bi-,tri-,fourgrams of nta with yta\n",
    "# very\n",
    "\n",
    "filtered_bigrams = [bg for bg in all_bigrams_nta if bg not in all_bigrams_yta_set]\n",
    "print(\"bigrams done\")\n",
    "filtered_bigrams = [bg for bg in all_trigrams_yta if bg not in all_trigrams_yta_set]\n",
    "print(\"trigrams done\")\n",
    "filtered_bigrams = [bg for bg in all_fourgrams_yta if bg not in all_fourgrams_yta_set]\n",
    "print(\"fourgrams done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7253ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the Counter class from Collections to find the top N most occurring Ngrams in our dataset\n",
    "top_bigrams = Counter(all_bigrams_nta).most_common(20)\n",
    "top_trigrams = Counter(all_trigrams_nta).most_common(20)\n",
    "top_fourgrams = Counter(all_fourgrams_nta).most_common(20)\n",
    "\n",
    "print(top_bigrams)\n",
    "print(top_trigrams)\n",
    "print(top_fourgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834d55a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter al the bi-,tri-,fourgrams of yta with nta\n",
    "# very slow\n",
    "\n",
    "all_bigrams_nta_set = list(set(all_bigrams_nta))\n",
    "all_trigrams_nta_set = list(set(all_trigrams_nta))\n",
    "all_fourgrams_nta_set = list(set(all_fourgrams_nta))\n",
    "\n",
    "filtered_bigrams = [bg for bg in all_bigrams_yta if bg not in all_bigrams_nta_set]\n",
    "print(\"bigrams done\")\n",
    "filtered_bigrams = [bg for bg in all_trigrams_yta if bg not in all_trigrams_nta_set]\n",
    "print(\"trigrams done\")\n",
    "filtered_bigrams = [bg for bg in all_fourgrams_yta if bg not in all_fourgrams_nta_set]\n",
    "print(\"fourgrams done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b9608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the Counter class from Collections to find the top N most occurring Ngrams in our dataset\n",
    "top_bigrams = Counter(all_bigrams_yta).most_common(20)\n",
    "top_trigrams = Counter(all_trigrams_yta).most_common(20)\n",
    "top_fourgrams = Counter(all_fourgrams_yta).most_common(20)\n",
    "\n",
    "print(top_bigrams)\n",
    "print(top_trigrams)\n",
    "print(top_fourgrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ti3160tu-nlp-group-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
