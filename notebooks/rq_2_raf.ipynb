{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f57099",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df7e8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Iterable, Dict, List, Tuple, Optional\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# display prefs\n",
    "pd.set_option('display.max_colwidth', 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb5629",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fa72689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "data_dir = Path('data')\n",
    "comments_file = data_dir / 'amitheasshole_comments.ndjson'\n",
    "submissions_file = data_dir / 'amitheasshole_submissions.ndjson'\n",
    "\n",
    "remake_datafile = False  # set True to force rebuilds\n",
    "\n",
    "interim_dir = Path('data/parquet')\n",
    "artifacts_dir = Path('artifacts')\n",
    "for p in [interim_dir, artifacts_dir]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# parquet outputs\n",
    "submissions_parquet = interim_dir / 'submissions_minimal.parquet'\n",
    "joined_dir = interim_dir / 'joined_parquet'\n",
    "\n",
    "# peek settings\n",
    "peek_n = 1_000\n",
    "\n",
    "# modelling sample size\n",
    "submission_sample_n = 1_000\n",
    "random_seed = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a5006",
   "metadata": {},
   "source": [
    "## ndjson loading and displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad38763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the comments file is ~22 GB; never load fully into RAM. stream lines lazily.\n",
    "\n",
    "def iter_ndjson_lines(path: Path) -> Iterator[dict]:\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # corrupted line guard; skip\n",
    "                continue\n",
    "\n",
    "def peek_ndjson(path: Path, n: int) -> pd.DataFrame:\n",
    "    # small materialisation to learn schema\n",
    "    rows = []\n",
    "    for i, obj in enumerate(iter_ndjson_lines(path)):\n",
    "        rows.append(obj)\n",
    "        if i + 1 >= n:\n",
    "            break\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def normalize_submission_id_from_link_id(link_id: str) -> Optional[str]:\n",
    "    # comments store parent submission as 't3_<id>'; we need bare '<id>'\n",
    "    if not link_id:\n",
    "        return None\n",
    "    parts = link_id.split('_', 1)\n",
    "    return parts[1] if len(parts) == 2 else link_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc38151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_peek = peek_ndjson(submissions_file, peek_n)\n",
    "df_com_peek = peek_ndjson(comments_file, peek_n)\n",
    "\n",
    "print('submissions columns:', sorted(df_sub_peek.columns.tolist()))\n",
    "print('comments columns:', sorted(df_com_peek.columns.tolist()))\n",
    "\n",
    "display(df_sub_peek.head(5))\n",
    "display(df_com_peek.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4359466",
   "metadata": {},
   "source": [
    "## Load, convert and combine submisions and comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2592ca15",
   "metadata": {},
   "source": [
    "#### submissions to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shrink submissions to only the fields we need and store them in a fast format\n",
    "sub_fields = ['id', 'title', 'selftext', 'link_flair_text', 'created_utc']\n",
    "\n",
    "def stream_submissions_to_parquet(src: Path, dst: Path, fields=sub_fields, batch_size: int = 100_000):\n",
    "    writer = None\n",
    "    rows = []\n",
    "    with src.open('r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            rows.append({k: obj.get(k) for k in fields})\n",
    "            if len(rows) >= batch_size:\n",
    "                df = pd.DataFrame(rows)\n",
    "                table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "                if writer is None:\n",
    "                    writer = pq.ParquetWriter(dst, table.schema)\n",
    "                writer.write_table(table)\n",
    "                rows.clear()\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows)\n",
    "            table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(dst, table.schema)\n",
    "            writer.write_table(table)\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "if remake_datafile or not submissions_parquet.exists():\n",
    "    submissions_parquet.parent.mkdir(parents=True, exist_ok=True)\n",
    "    stream_submissions_to_parquet(submissions_file, submissions_parquet)\n",
    "    print('wrote:', submissions_parquet)\n",
    "else:\n",
    "    print('exists:', submissions_parquet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c3422",
   "metadata": {},
   "source": [
    "#### load submissions into ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04159be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_parquet(submissions_parquet, columns=['id', 'title', 'selftext', 'link_flair_text', 'created_utc'])\n",
    "sub_df = sub_df.dropna(subset=['id']).drop_duplicates(subset=['id'])\n",
    "sub_df = sub_df.set_index('id', drop=True)\n",
    "approx_mb = sub_df.memory_usage(deep=True).sum() / 1e6\n",
    "print('submissions frame:', sub_df.shape, f'~{approx_mb:.1f} MB in RAM')\n",
    "display(sub_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d404fab2",
   "metadata": {},
   "source": [
    "#### hierarchical comments and submissions parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d324ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a hierarchical table directly from the raw comments + in-memory submissions index (sub_df)\n",
    "\n",
    "hier_parquet = interim_dir / 'submissions_hierarchical.parquet'\n",
    "\n",
    "# knobs\n",
    "include_op_only = False  # set True to only keep OP (is_submitter == True) comments\n",
    "\n",
    "def read_parquet_nested_safe(path: Path) -> pd.DataFrame:\n",
    "    # why: safely load parquet with list (nested) columns\n",
    "    table = pq.read_table(path)\n",
    "    table = table.combine_chunks()\n",
    "    return table.to_pandas()\n",
    "\n",
    "# type-casting helpers to enforce homogeneous list element types\n",
    "def _to_str(x):\n",
    "    return None if x is None else str(x)\n",
    "\n",
    "def _to_int(x):\n",
    "    if x is None: return None\n",
    "    try: return int(x)\n",
    "    except Exception: return None\n",
    "\n",
    "def _to_float(x):\n",
    "    if x is None: return None\n",
    "    try: return float(x)\n",
    "    except Exception: return None\n",
    "\n",
    "def _to_bool(x):\n",
    "    if x is None: return None\n",
    "    return bool(x)\n",
    "\n",
    "def build_hierarchical_direct(comments_path: Path,\n",
    "                              sub_index_df: pd.DataFrame,\n",
    "                              out_path: Path,\n",
    "                              include_op_only: bool = False,\n",
    "                              limit_lines: Optional[int] = None) -> None:\n",
    "    \"\"\"\n",
    "    Stream comments.ndjson and build a hierarchical table with one row per submission\n",
    "    and *typed* list columns of comments (no cap). Writes with an explicit Arrow schema\n",
    "    using LARGE string/list types to avoid 2GB offset overflow.\n",
    "    \"\"\"\n",
    "    comments_by_sub: Dict[str, Dict[str, list]] = {}\n",
    "    sub_meta: Dict[str, Dict[str, object]] = {}\n",
    "\n",
    "    have_sub = sub_index_df.index\n",
    "    get_row = sub_index_df.loc\n",
    "\n",
    "    with comments_path.open('r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            if limit_lines and i > limit_lines:\n",
    "                break\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                c = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            sub_id = normalize_submission_id_from_link_id(c.get('link_id', ''))\n",
    "            if not sub_id or sub_id not in have_sub:\n",
    "                continue\n",
    "\n",
    "            if include_op_only and not c.get('is_submitter', False):\n",
    "                continue\n",
    "\n",
    "            if sub_id not in sub_meta:\n",
    "                s = get_row[sub_id]\n",
    "                sub_meta[sub_id] = {\n",
    "                    'submission_id': _to_str(sub_id),\n",
    "                    'submission_title': _to_str(s.get('title')),\n",
    "                    'submission_selftext': _to_str(s.get('selftext')),\n",
    "                    'submission_flair': _to_str(s.get('link_flair_text')),\n",
    "                    'submission_created_utc': _to_float(s.get('created_utc')),\n",
    "                }\n",
    "\n",
    "            if sub_id not in comments_by_sub:\n",
    "                comments_by_sub[sub_id] = {\n",
    "                    'comment_ids': [],\n",
    "                    'comment_bodies': [],\n",
    "                    'comment_scores': [],\n",
    "                    'comment_created_utcs': [],\n",
    "                    'is_submitters': []\n",
    "                }\n",
    "\n",
    "            buf = comments_by_sub[sub_id]\n",
    "            buf['comment_ids'].append(_to_str(c.get('id')))\n",
    "            buf['comment_bodies'].append(_to_str(c.get('body')))\n",
    "            buf['comment_scores'].append(_to_int(c.get('score')))\n",
    "            buf['comment_created_utcs'].append(_to_float(c.get('created_utc')))\n",
    "            buf['is_submitters'].append(_to_bool(c.get('is_submitter')))\n",
    "\n",
    "    # materialise to Arrow directly (skip pandas for writing nested columns)\n",
    "    rows = []\n",
    "    for sid, meta in sub_meta.items():\n",
    "        buf = comments_by_sub.get(sid, {\n",
    "            'comment_ids': [], 'comment_bodies': [], 'comment_scores': [],\n",
    "            'comment_created_utcs': [], 'is_submitters': []\n",
    "        })\n",
    "        rows.append({\n",
    "            **meta,\n",
    "            'comment_ids': buf['comment_ids'],\n",
    "            'comment_bodies': buf['comment_bodies'],\n",
    "            'comment_scores': buf['comment_scores'],\n",
    "            'comment_created_utcs': buf['comment_created_utcs'],\n",
    "            'is_submitters': buf['is_submitters'],\n",
    "            'n_comments': len(buf['comment_ids'])\n",
    "        })\n",
    "\n",
    "    # use LARGE string/list types to avoid offset overflow\n",
    "    STR = pa.large_string()\n",
    "    LSTR_LIST = pa.large_list(STR)\n",
    "\n",
    "    schema = pa.schema([\n",
    "        pa.field('submission_id', STR),\n",
    "        pa.field('submission_title', STR),\n",
    "        pa.field('submission_selftext', STR),\n",
    "        pa.field('submission_flair', STR),\n",
    "        pa.field('submission_created_utc', pa.float64()),\n",
    "        pa.field('comment_ids', LSTR_LIST),\n",
    "        pa.field('comment_bodies', LSTR_LIST),\n",
    "        pa.field('comment_scores', pa.list_(pa.int64())),\n",
    "        pa.field('comment_created_utcs', pa.list_(pa.float64())),\n",
    "        pa.field('is_submitters', pa.list_(pa.bool_())),\n",
    "        pa.field('n_comments', pa.int64()),\n",
    "    ])\n",
    "\n",
    "    def col(name, typ):\n",
    "        return pa.array([row.get(name) for row in rows], type=typ)\n",
    "\n",
    "    table = pa.table([\n",
    "        col('submission_id', STR),\n",
    "        col('submission_title', STR),\n",
    "        col('submission_selftext', STR),\n",
    "        col('submission_flair', STR),\n",
    "        col('submission_created_utc', pa.float64()),\n",
    "        col('comment_ids', LSTR_LIST),\n",
    "        col('comment_bodies', LSTR_LIST),\n",
    "        col('comment_scores', pa.list_(pa.int64())),\n",
    "        col('comment_created_utcs', pa.list_(pa.float64())),\n",
    "        col('is_submitters', pa.list_(pa.bool_())),\n",
    "        col('n_comments', pa.int64()),\n",
    "    ], schema=schema)\n",
    "\n",
    "    pq.write_table(table, out_path)\n",
    "    print(f'wrote hierarchical table (direct, typed): {out_path} ({table.num_rows} submissions)')\n",
    "\n",
    "# build-or-load guard (no need for an intermediate \"joined_*\" step)\n",
    "if remake_datafile or not hier_parquet.exists():\n",
    "    build_hierarchical_direct(\n",
    "        comments_path=comments_file,\n",
    "        sub_index_df=sub_df,          # from block 6 (indexed by 'id')\n",
    "        out_path=hier_parquet,\n",
    "        include_op_only=include_op_only,\n",
    "        limit_lines=None              # set e.g. 2_000 for a quick dry-run\n",
    "    )\n",
    "    # preview safely by reading only a projection (tiny) to avoid heavy nested ops\n",
    "    preview = pq.read_table(hier_parquet, columns=['submission_id', 'n_comments']).slice(0, 2).to_pandas()\n",
    "    display(preview)\n",
    "else:\n",
    "    hier_df = read_parquet_nested_safe(hier_parquet)\n",
    "    print(f'using existing hierarchical file: {hier_parquet} ({len(hier_df)} submissions)')\n",
    "    display(hier_df.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d4a85a",
   "metadata": {},
   "source": [
    "## Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02f4d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_spacy(nlp_name: str = 'en_core_web_sm'):\n",
    "    try:\n",
    "        return spacy.load(nlp_name, disable=['parser', 'textcat'])\n",
    "    except OSError as e:\n",
    "        raise RuntimeError(\n",
    "            f\"spaCy model {nlp_name!r} not installed. run: python -m spacy download {nlp_name}\"\n",
    "        ) from e\n",
    "\n",
    "nlp = ensure_spacy()\n",
    "\n",
    "url_re = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "nonword_re = re.compile(r\"[^a-zA-Z']+\")\n",
    "multispace_re = re.compile(r'\\s+')\n",
    "\n",
    "def clean_text(text: Optional[str]) -> str:\n",
    "    if not text:\n",
    "        return ''\n",
    "    text = url_re.sub(' ', text)\n",
    "    text = text.lower()\n",
    "    text = nonword_re.sub(' ', text)\n",
    "    text = multispace_re.sub(' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def lemmatize(text: str, nlp=nlp, do_ner: bool = True) -> Tuple[str, List[str]]:\n",
    "    if not text:\n",
    "        return '', []\n",
    "    doc = nlp(text)\n",
    "    lemmas = [t.lemma_ for t in doc if not (t.is_stop or t.is_punct or t.is_space)]\n",
    "    ents = [f'{ent.label_}:{ent.text}' for ent in doc.ents] if do_ner else []\n",
    "    return ' '.join(lemmas), ents\n",
    "\n",
    "def preprocess_submission_row(row: dict, do_ner: bool = True) -> dict:\n",
    "    raw = ' '.join([str(row.get('title') or ''), str(row.get('selftext') or '')]).strip()\n",
    "    cleaned = clean_text(raw)\n",
    "    lemmas, ents = lemmatize(cleaned, do_ner=do_ner)\n",
    "    return {\n",
    "        'id': row.get('id'),\n",
    "        'flair': row.get('link_flair_text'),\n",
    "        'created_utc': row.get('created_utc'),\n",
    "        'text_raw': raw,\n",
    "        'text_clean': cleaned,\n",
    "        'text_lemmas': lemmas,\n",
    "        'ents': ents\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a996c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why: filter to usable posts once (before any NER) and cache them for fast reloads later.\n",
    "#      This creates:\n",
    "#        1) submissions_usable.parquet  (submissions only)\n",
    "#        2) submissions_with_comments_usable.parquet  (from hierarchical: title/body/comments)\n",
    "\n",
    "# thresholds\n",
    "min_body_len = 50       # chars required in submission selftext\n",
    "min_title_len = 40      # chars required in title\n",
    "min_comment_len = 150   # total chars across all comments (hierarchical path)\n",
    "\n",
    "removed_markers = {'[removed]', '[deleted]', None, ''}\n",
    "\n",
    "def is_removed(txt):\n",
    "    return (txt is None) or (str(txt).strip() in removed_markers)\n",
    "\n",
    "def text_len(s):\n",
    "    return 0 if s is None else len(str(s).strip())\n",
    "\n",
    "def total_comment_chars(bodies):\n",
    "    if not isinstance(bodies, list):\n",
    "        return 0\n",
    "    return sum(len(str(b or '')) for b in bodies)\n",
    "\n",
    "# ---------- submissions-only usable ----------\n",
    "# diagnostics on the full submissions index frame\n",
    "diag = pd.DataFrame({\n",
    "    'has_body': ~sub_df['selftext'].apply(is_removed),\n",
    "    'title_len': sub_df['title'].apply(text_len),\n",
    "    'body_len': sub_df['selftext'].apply(text_len),\n",
    "    'flair_none': sub_df['link_flair_text'].isna()\n",
    "})\n",
    "print('total submissions:', len(sub_df))\n",
    "print('with usable body:', int(diag['has_body'].sum()))\n",
    "print('flair available:', int((~diag[\"flair_none\"]).sum()))\n",
    "display(diag.describe())\n",
    "\n",
    "# keep if (body usable & body_len >= min_body_len) OR (title_len >= min_title_len)\n",
    "usable_mask = (\n",
    "    (~sub_df['selftext'].apply(is_removed) & (sub_df['selftext'].apply(text_len) >= min_body_len))\n",
    "    | (sub_df['title'].apply(text_len) >= min_title_len)\n",
    ")\n",
    "usable_sub_df = sub_df.loc[usable_mask, ['title','selftext','link_flair_text','created_utc']].copy()\n",
    "print('usable submissions (submissions-only):', len(usable_sub_df))\n",
    "\n",
    "# cache (submissions only)\n",
    "usable_submissions_parquet = interim_dir / 'submissions_usable.parquet'\n",
    "# uncomment if you want them saved:\n",
    "# usable_sub_df.to_parquet(usable_submissions_parquet, index=True)\n",
    "# print('saved:', usable_submissions_parquet)\n",
    "\n",
    "\n",
    "# ---------- hierarchical usable (submissions + comments) ----------\n",
    "# build from hierarchical parquet\n",
    "usable_hier_parquet = interim_dir / 'submissions_with_comments_usable.parquet'\n",
    "\n",
    "if 'hier_df' in globals() or (('hier_parquet' in globals()) and Path(hier_parquet).exists()):\n",
    "    # load hier_df if not already in memory\n",
    "    if 'hier_df' not in globals():\n",
    "        hier_df = pd.read_parquet(hier_parquet)\n",
    "\n",
    "    # usable if any of: selftext long enough, title long enough, OR enough total comment text\n",
    "    usable_hier_mask = (\n",
    "        ((~hier_df['submission_selftext'].apply(is_removed)) & (hier_df['submission_selftext'].apply(text_len) >= min_body_len))\n",
    "        | (hier_df['submission_title'].apply(text_len) >= min_title_len)\n",
    "        | (hier_df['comment_bodies'].apply(total_comment_chars) >= min_comment_len)\n",
    "    )\n",
    "    usable_hier_df = hier_df.loc[usable_hier_mask].copy()\n",
    "    print('usable submissions (with comments):', len(usable_hier_df), 'of', len(hier_df))\n",
    "\n",
    "    # save a compact projection that’s all we need for later preprocessing-from-hierarchical\n",
    "    cols_keep = [\n",
    "        'submission_id','submission_title','submission_selftext',\n",
    "        'submission_flair','submission_created_utc',\n",
    "        'comment_bodies','is_submitters','n_comments'\n",
    "    ]\n",
    "    # uncomment if you want them saved:\n",
    "    # usable_hier_df[cols_keep].to_parquet(usable_hier_parquet, index=False)\n",
    "    # print('saved:', usable_hier_parquet)\n",
    "\n",
    "    # quick peek\n",
    "    display(usable_hier_df.head(2)[['submission_id','n_comments']])\n",
    "else:\n",
    "    print('hierarchical parquet not found — skipping submissions+comments usable cache. '\n",
    "          'Build it first with build_hierarchical_direct().')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855144c",
   "metadata": {},
   "source": [
    "#### apply preproccessing to sumbitions and save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b555dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why: turn usable submissions into a clean, lemmatised, NER-tagged dataset and cache it for fast reloads\n",
    "\n",
    "submissions_preprocessed_parquet = interim_dir / 'submissions_preprocessed.parquet'\n",
    "build_submissions_preprocessed = True          # set False to skip rebuilding if file exists\n",
    "overwrite_submissions_preprocessed = False     # set True to force overwrite\n",
    "\n",
    "def preprocess_submissions_df(df_sub_indexed: pd.DataFrame, do_ner: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    df_sub_indexed: dataframe indexed by 'id' with columns ['title','selftext','link_flair_text','created_utc']\n",
    "    returns: dataframe with ['id','flair','created_utc','text_raw','text_clean','text_lemmas','ents']\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for sid, row in df_sub_indexed[['title','selftext','link_flair_text','created_utc']].iterrows():\n",
    "        row_adapted = {\n",
    "            'id': sid,\n",
    "            'title': row.get('title'),\n",
    "            'selftext': row.get('selftext'),\n",
    "            'link_flair_text': row.get('link_flair_text'),\n",
    "            'created_utc': row.get('created_utc'),\n",
    "        }\n",
    "        records.append(preprocess_submission_row(row_adapted, do_ner=do_ner))\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "if submissions_preprocessed_parquet.exists() and not overwrite_submissions_preprocessed and not build_submissions_preprocessed:\n",
    "    submissions_preprocessed_df = pd.read_parquet(submissions_preprocessed_parquet)\n",
    "    print('loaded:', submissions_preprocessed_parquet, f'({len(submissions_preprocessed_df)} rows)')\n",
    "else:\n",
    "    # source is the cached usable submissions from block 9 (indexed by 'id')\n",
    "    if 'usable_sub_df' not in globals():\n",
    "        raise RuntimeError('usable_sub_df not found. Run block 9 first to create/load it.')\n",
    "    submissions_preprocessed_df = preprocess_submissions_df(usable_sub_df, do_ner=True)\n",
    "    submissions_preprocessed_df.to_parquet(submissions_preprocessed_parquet, index=False)\n",
    "    print('saved:', submissions_preprocessed_parquet, f'({len(submissions_preprocessed_df)} rows)')\n",
    "\n",
    "display(submissions_preprocessed_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ad4fd",
   "metadata": {},
   "source": [
    "#### apply preproccessing to sumbitions+comments and save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab190c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why: turn hierarchical rows (title + selftext + all comments) into cleaned/lemmatised/NER'd text,\n",
    "#      but do it in chunks to avoid RAM blowups and to allow resuming.\n",
    "\n",
    "import math\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "parts_dir = interim_dir / 'submissions_with_all_comments_preprocessed_parts'\n",
    "parts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_merged_parquet = interim_dir / 'submissions_with_all_comments_preprocessed.parquet'\n",
    "use_op_only_text = False        # True = only include OP comments in the text\n",
    "batch_rows = 1_000              # tune based on CPU/RAM\n",
    "resume = True                   # skip batches that already have a written part file\n",
    "\n",
    "def ensure_list_of_str(x):\n",
    "    # why: robustly coerce hierarchical column to a list[str]\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return [str(v) if v is not None else '' for v in x]\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return [str(v) if v is not None else '' for v in x.tolist()]\n",
    "    # sometimes a scalar sneaks in; treat as single-comment list\n",
    "    if isinstance(x, str):\n",
    "        return [x]\n",
    "    try:\n",
    "        return [str(v) for v in list(x)]\n",
    "    except Exception:\n",
    "        return [str(x)]\n",
    "\n",
    "def iter_hier_batches(path: Path, columns=None, batch_size: int = 1000):\n",
    "    \"\"\"\n",
    "    Stream the hierarchical parquet in record batches using ParquetFile.iter_batches,\n",
    "    yielding small pandas DataFrames. Works across PyArrow versions.\n",
    "    \"\"\"\n",
    "    pf = pq.ParquetFile(str(path))\n",
    "    # iter_batches returns pyarrow.RecordBatch objects in chunks\n",
    "    for batch in pf.iter_batches(batch_size=batch_size, columns=columns):\n",
    "        yield batch.to_pandas()\n",
    "\n",
    "\n",
    "def preprocess_hierarchical_in_chunks(hier_path: Path,\n",
    "                                      out_dir: Path,\n",
    "                                      batch_rows: int = 1000,\n",
    "                                      op_only: bool = False,\n",
    "                                      resume: bool = True):\n",
    "    cols = [\n",
    "        'submission_id','submission_title','submission_selftext',\n",
    "        'submission_flair','submission_created_utc',\n",
    "        'comment_bodies','is_submitters','n_comments'\n",
    "    ]\n",
    "    part_idx = 0\n",
    "    for pdf in iter_hier_batches(hier_path, columns=cols, batch_size=batch_rows):\n",
    "        part_path = out_dir / f'part-{part_idx:05d}.parquet'\n",
    "        if resume and part_path.exists():\n",
    "            part_idx += 1\n",
    "            continue\n",
    "\n",
    "        # build raw text (title + selftext + comments)\n",
    "        all_raw = []\n",
    "        n_used = []\n",
    "        for r in pdf.itertuples(index=False):\n",
    "            bodies = ensure_list_of_str(r.comment_bodies)\n",
    "            if op_only:\n",
    "                flags = ensure_list_of_str(r.is_submitters)  # may still be list/ndarray of bools; ensure_list gives strings\n",
    "                # convert flags back to bools where possible\n",
    "                flags_bool = []\n",
    "                for v in (r.is_submitters if isinstance(r.is_submitters, list) else (r.is_submitters.tolist() if isinstance(r.is_submitters, np.ndarray) else [])):\n",
    "                    flags_bool.append(bool(v))\n",
    "                if flags_bool:\n",
    "                    bodies = [b for b, f in zip(bodies, flags_bool) if f]\n",
    "            n_used.append(len(bodies))\n",
    "            raw = ' '.join([\n",
    "                str(r.submission_title or ''),\n",
    "                str(r.submission_selftext or ''),\n",
    "                ' '.join(bodies)\n",
    "            ]).strip()\n",
    "            all_raw.append(raw)\n",
    "\n",
    "        # clean first (cheap), then spaCy once via pipe\n",
    "        cleaned = [clean_text(t) for t in all_raw]\n",
    "        docs = list(nlp.pipe(cleaned, batch_size=64, n_process=1))\n",
    "        lemmas = [' '.join(t.lemma_ for t in doc if not (t.is_stop or t.is_punct or t.is_space)) for doc in docs]\n",
    "        ents = [[f'{e.label_}:{e.text}' for e in doc.ents] for doc in docs]\n",
    "\n",
    "        out_df = pd.DataFrame({\n",
    "            'id': pdf['submission_id'].astype(str).values,\n",
    "            'flair': pdf['submission_flair'].astype('string').where(pdf['submission_flair'].notna(), None),\n",
    "            'created_utc': pdf['submission_created_utc'].astype(float).values,\n",
    "            'text_raw': all_raw,\n",
    "            'text_clean': cleaned,\n",
    "            'text_lemmas': lemmas,\n",
    "            'ents': ents,\n",
    "            'n_comments': pdf['n_comments'].astype('Int64').values,\n",
    "            'n_comment_bodies_used': n_used\n",
    "        })\n",
    "\n",
    "        # write this chunk (use Arrow schema with large strings to be safe)\n",
    "        schema = pa.schema([\n",
    "            pa.field('id', pa.string()),\n",
    "            pa.field('flair', pa.string()),\n",
    "            pa.field('created_utc', pa.float64()),\n",
    "            pa.field('text_raw', pa.large_string()),\n",
    "            pa.field('text_clean', pa.large_string()),\n",
    "            pa.field('text_lemmas', pa.large_string()),\n",
    "            pa.field('ents', pa.list_(pa.string())),\n",
    "            pa.field('n_comments', pa.int64()),\n",
    "            pa.field('n_comment_bodies_used', pa.int64()),\n",
    "        ])\n",
    "        pq.write_table(pa.Table.from_pandas(out_df, schema=schema, preserve_index=False),\n",
    "                       part_path)\n",
    "        print(f'wrote {part_path} ({len(out_df)} rows)')\n",
    "        part_idx += 1\n",
    "\n",
    "    print('done preprocessing hierarchical in chunks.')\n",
    "\n",
    "def merge_preprocessed_parts(parts_dir: Path, out_path: Path):\n",
    "    # why: merge part files into a single parquet (optional; you can also keep the dir as a dataset)\n",
    "    parts = sorted(parts_dir.glob('part-*.parquet'))\n",
    "    if not parts:\n",
    "        print('no parts to merge.')\n",
    "        return\n",
    "    tables = [pq.read_table(p) for p in parts]\n",
    "    pq.write_table(pa.concat_tables(tables, promote=True), out_path)\n",
    "    print('merged', len(parts), 'parts into', out_path)\n",
    "\n",
    "# run it\n",
    "if 'usable_hier_df' in globals():\n",
    "    # if you built a usable subset parquet in block 9, use that file to stream\n",
    "    hier_source = interim_dir / 'submissions_with_comments_usable.parquet'\n",
    "    if not hier_source.exists():\n",
    "        # fallback to full hierarchical if usable subset file not written\n",
    "        hier_source = hier_parquet\n",
    "else:\n",
    "    hier_source = hier_parquet\n",
    "\n",
    "preprocess_hierarchical_in_chunks(\n",
    "    hier_path=hier_source,\n",
    "    out_dir=parts_dir,\n",
    "    batch_rows=batch_rows,\n",
    "    op_only=use_op_only_text,\n",
    "    resume=resume\n",
    ")\n",
    "\n",
    "# optional: merge all parts into a single file (can skip if you like dataset-of-parts)\n",
    "# merge_preprocessed_parts(parts_dir, out_merged_parquet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed542479",
   "metadata": {},
   "source": [
    "## Sample submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9898a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fetch_random_submissions_df_from_usable(usable_index_df: pd.DataFrame, n: int, seed: int = 42) -> pd.DataFrame:\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     ids = usable_index_df.index.values\n",
    "#     pick = ids if n >= len(ids) else rng.choice(ids, size=n, replace=False)\n",
    "#     df = usable_index_df.loc[pick, ['title', 'selftext', 'link_flair_text', 'created_utc']].reset_index()\n",
    "#     df = df.rename(columns={'index': 'id'})\n",
    "#     return df\n",
    "\n",
    "# def build_corpus(df_sub: pd.DataFrame, do_ner: bool = True) -> pd.DataFrame:\n",
    "#     processed = [preprocess_submission_row(row, do_ner=do_ner) for row in df_sub.to_dict('records')]\n",
    "#     return pd.DataFrame(processed)\n",
    "\n",
    "# df_sub_sample = fetch_random_submissions_df_from_usable(usable_sub_df, submission_sample_n, seed=random_seed)\n",
    "# print('submissions sampled for modelling (usable only):', len(df_sub_sample))\n",
    "# display(df_sub_sample.head(3))\n",
    "\n",
    "# df_corpus = build_corpus(df_sub_sample, do_ner=True)\n",
    "# display(df_corpus.head(3)[['id', 'flair', 'text_raw']])\n",
    "\n",
    "df_corpus = submissions_preprocessed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c01940",
   "metadata": {},
   "source": [
    "## LDA topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce45d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit LDA on lemmatised bag-of-words\n",
    "def fit_lda(texts: Iterable[str], max_features: int = 50_000, n_topics: int = 15, max_df: float = 0.5, min_df: int = 10, random_state: int = 42):\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_features=max_features,\n",
    "        stop_words='english',\n",
    "        max_df=max_df,\n",
    "        min_df=min_df\n",
    "    )\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    lda = LDA(n_components=n_topics, random_state=random_state, learning_method='batch')\n",
    "    W = lda.fit_transform(X)  # doc-topic matrix\n",
    "    H = lda.components_        # topic-term matrix\n",
    "    return lda, vectorizer, W, H\n",
    "\n",
    "lda, vect, W, H = fit_lda(df_corpus['text_lemmas'].tolist(), n_topics=15, random_state=random_seed)\n",
    "\n",
    "def top_words_per_topic(vect, H, topn: int = 15) -> List[List[str]]:\n",
    "    feature_names = np.array(vect.get_feature_names_out())\n",
    "    topics = []\n",
    "    for k in range(H.shape[0]):\n",
    "        idx = np.argsort(H[k])[::-1][:topn]\n",
    "        topics.append(feature_names[idx].tolist())\n",
    "    return topics\n",
    "\n",
    "topics_top_words = top_words_per_topic(vect, H, topn=15)\n",
    "for i, words in enumerate(topics_top_words):\n",
    "    print(f'topic {i:02d}:', ', '.join(words))\n",
    "\n",
    "# map topics to 5 categories using keyword overlap\n",
    "category_keywords = {\n",
    "    'finances': {\n",
    "        'money','pay','paid','rent','bill','bills','loan','debt','card','credit','cash','salary','bonus','split','cost','expensive','cheap','wedding','gift','refund','share','finance'\n",
    "    },\n",
    "    'relationship': {\n",
    "        'relationship','boyfriend','girlfriend','partner','date','dating','romantic','love','cheat','ex','fiancé','fiance','fiancee','breakup','trust','jealous'\n",
    "    },\n",
    "    'family_conflict': {\n",
    "        'mom','dad','mother','father','sister','brother','siblings','family','cousin','aunt','uncle','inlaws','in','law','grandma','grandpa','child','baby','pregnant','wedding','name'\n",
    "    },\n",
    "    'work': {\n",
    "        'work','job','boss','coworker','manager','shift','hours','office','remote','payroll','promotion','hr','fire','fired','leave','paternity','maternity'\n",
    "    },\n",
    "    'societal_norms': {\n",
    "        'culture','religion','religious','tradition','gender','pronoun','politics','law','legal','illegal','discrimination','racist','ableist','ethics','value','norm','boundary','consent'\n",
    "    }\n",
    "}\n",
    "\n",
    "def score_topic_to_category(words: List[str]) -> Tuple[str, Dict[str, int]]:\n",
    "    scores = {cat: 0 for cat in category_keywords}\n",
    "    wordset = set(words)\n",
    "    for cat, kw in category_keywords.items():\n",
    "        scores[cat] = len(wordset & kw)\n",
    "    best_cat = max(scores, key=scores.get)\n",
    "    return best_cat, scores\n",
    "\n",
    "topic_category = []\n",
    "for i, words in enumerate(topics_top_words):\n",
    "    best, scores = score_topic_to_category(words)\n",
    "    topic_category.append({'topic': i, 'category': best, **scores})\n",
    "\n",
    "df_topic_map = pd.DataFrame(topic_category).sort_values(['category', 'topic'])\n",
    "display(df_topic_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d67d9f",
   "metadata": {},
   "source": [
    "## Assign topic with NER bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52493cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = np.argmax(W, axis=1)\n",
    "df_corpus['topic'] = topic_labels\n",
    "\n",
    "topic_to_cat = {row['topic']: row['category'] for _, row in df_topic_map.iterrows()}\n",
    "df_corpus['category_initial'] = df_corpus['topic'].map(topic_to_cat).fillna('societal_norms')\n",
    "\n",
    "def ner_bias_category(ents: List[str], current: str) -> str:\n",
    "    labels = [e.split(':', 1)[0] for e in ents]\n",
    "    if any(lbl in ('NORP', 'LAW') for lbl in labels) and current in ('relationship', 'work', 'finances'):\n",
    "        return 'societal_norms'\n",
    "    if any(lbl in ('PERSON',) for lbl in labels) and current == 'societal_norms':\n",
    "        return 'family_conflict'\n",
    "    return current\n",
    "\n",
    "df_corpus['category'] = [\n",
    "    ner_bias_category(ents, cat) for ents, cat in zip(df_corpus['ents'], df_corpus['category_initial'])\n",
    "]\n",
    "\n",
    "category_counts = df_corpus['category'].value_counts().rename_axis('category').reset_index(name='count')\n",
    "display(category_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340ed459",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a6bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examples_by_category(df: pd.DataFrame, cat: str, k: int = 5) -> pd.DataFrame:\n",
    "    ex = df.loc[df['category'] == cat, ['id', 'flair', 'text_raw']].head(k).copy()\n",
    "    return ex\n",
    "\n",
    "for cat in ['finances', 'relationship', 'family_conflict', 'work', 'societal_norms']:\n",
    "    print(f'\\n=== {cat.upper()} EXAMPLES ===')\n",
    "    display(examples_by_category(df_corpus, cat, k=5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ti3160tu-nlp-group-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
