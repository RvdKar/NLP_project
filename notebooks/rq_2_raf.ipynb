{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cff7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Iterable, Dict, List, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "except Exception as e:\n",
    "    raise RuntimeError('pyarrow is required. install with: pip install pyarrow') from e\n",
    "\n",
    "# display prefs\n",
    "pd.set_option('display.max_colwidth', 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1049a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "DATA_DIR = Path('data')\n",
    "COMMENTS_FILE = DATA_DIR / 'amitheasshole_comments.ndjson'\n",
    "SUBMISSIONS_FILE = DATA_DIR / 'amitheasshole_submissions.ndjson'\n",
    "\n",
    "remake_datafile=False\n",
    "\n",
    "INTERIM_DIR = Path('interim')\n",
    "ARTIFACTS_DIR = Path('artifacts')\n",
    "for p in [INTERIM_DIR, ARTIFACTS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# parquet dirs\n",
    "SUBMISSIONS_PARQUET = INTERIM_DIR / 'submissions_minimal.parquet'\n",
    "JOINED_DIR = INTERIM_DIR / 'joined_parquet'\n",
    "\n",
    "# peek settings\n",
    "PEEK_N = 1000 \n",
    "\n",
    "# modelling sample size\n",
    "SUBMISSION_SAMPLE_N = 1000\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb3541a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why: the comments file is ~22 GB; never load fully into RAM. stream lines lazily.\n",
    "\n",
    "def iter_ndjson_lines(path: Path) -> Iterator[dict]:\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # corrupted line guard; skip\n",
    "                continue\n",
    "\n",
    "\n",
    "def peek_ndjson(path: Path, n: int) -> pd.DataFrame:\n",
    "    # why: small materialisation to learn schema\n",
    "    rows = []\n",
    "    for i, obj in enumerate(iter_ndjson_lines(path)):\n",
    "        rows.append(obj)\n",
    "        if i + 1 >= n:\n",
    "            break\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def normalize_submission_id_from_link_id(link_id: str) -> Optional[str]:\n",
    "    # why: comments store parent submission as 't3_<id>'; we need bare '<id>'\n",
    "    if not link_id:\n",
    "        return None\n",
    "    parts = link_id.split('_', 1)\n",
    "    return parts[1] if len(parts) == 2 else link_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0494ff6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submissions columns: ['all_awardings', 'allow_live_comments', 'archived', 'author', 'author_cakeday', 'author_created_utc', 'author_flair_background_color', 'author_flair_css_class', 'author_flair_richtext', 'author_flair_template_id', 'author_flair_text', 'author_flair_text_color', 'author_flair_type', 'author_fullname', 'author_patreon_flair', 'author_premium', 'awarders', 'banned_by', 'call_to_action', 'can_gild', 'can_mod_post', 'category', 'content_categories', 'contest_mode', 'created_utc', 'discussion_type', 'distinguished', 'domain', 'edited', 'gilded', 'gildings', 'hidden', 'hide_score', 'id', 'is_created_from_ads_ui', 'is_crosspostable', 'is_meta', 'is_original_content', 'is_reddit_media_domain', 'is_robot_indexable', 'is_self', 'is_video', 'link_flair_background_color', 'link_flair_css_class', 'link_flair_richtext', 'link_flair_template_id', 'link_flair_text', 'link_flair_text_color', 'link_flair_type', 'locked', 'media', 'media_embed', 'media_only', 'name', 'no_follow', 'num_comments', 'num_crossposts', 'over_18', 'parent_whitelist_status', 'permalink', 'pinned', 'pwls', 'quarantine', 'removed_by', 'removed_by_category', 'retrieved_on', 'retrieved_utc', 'score', 'secure_media', 'secure_media_embed', 'selftext', 'send_replies', 'spoiler', 'stickied', 'subreddit', 'subreddit_id', 'subreddit_name_prefixed', 'subreddit_subscribers', 'subreddit_type', 'suggested_sort', 'thumbnail', 'thumbnail_height', 'thumbnail_width', 'title', 'top_awarded_type', 'total_awards_received', 'treatment_tags', 'upvote_ratio', 'url', 'view_count', 'whitelist_status', 'wls']\n",
      "comments columns: ['all_awardings', 'archived', 'associated_award', 'author', 'author_cakeday', 'author_created_utc', 'author_flair_background_color', 'author_flair_css_class', 'author_flair_richtext', 'author_flair_template_id', 'author_flair_text', 'author_flair_text_color', 'author_flair_type', 'author_fullname', 'author_patreon_flair', 'author_premium', 'body', 'can_gild', 'collapsed', 'collapsed_because_crowd_control', 'collapsed_reason', 'collapsed_reason_code', 'comment_type', 'controversiality', 'created_utc', 'distinguished', 'edited', 'gilded', 'gildings', 'id', 'is_submitter', 'link_id', 'locked', 'name', 'no_follow', 'parent_id', 'permalink', 'retrieved_on', 'score', 'score_hidden', 'send_replies', 'stickied', 'subreddit', 'subreddit_id', 'subreddit_name_prefixed', 'subreddit_type', 'top_awarded_type', 'total_awards_received', 'treatment_tags', 'unrepliable_reason']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_created_utc</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>...</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>treatment_tags</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>url</th>\n",
       "      <th>view_count</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "      <th>link_flair_template_id</th>\n",
       "      <th>call_to_action</th>\n",
       "      <th>author_cakeday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beanstressed</td>\n",
       "      <td>1.627668e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.71</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comments/v2fbg0/wibta_if_i_get_my_hair_braided/</td>\n",
       "      <td>None</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Good-Barracuda5143</td>\n",
       "      <td>1.609642e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comments/v2fdaf/aita_for_uninviting_a_best_friend_to_my_gender/</td>\n",
       "      <td>None</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.77</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comments/v2fdq0/aita_for_being_mad_at_my_friends_for_not_sticking/</td>\n",
       "      <td>None</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>20701dd2-d245-11e8-99f1-0e2d925c15f4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>LisKoz1989</td>\n",
       "      <td>1.654004e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.90</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comments/v2fgt3/aita_for_never_wanting_to_see_a_guy_after_he_lied/</td>\n",
       "      <td>None</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comments/v2fh5n/aita_for_not_wanting_my_boyfriends_sister_to_live/</td>\n",
       "      <td>None</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments  archived              author  \\\n",
       "0            []                False     False        beanstressed   \n",
       "1            []                False     False  Good-Barracuda5143   \n",
       "2            []                 True     False           [deleted]   \n",
       "3            []                False     False          LisKoz1989   \n",
       "4            []                False     False           [deleted]   \n",
       "\n",
       "   author_created_utc author_flair_background_color author_flair_css_class  \\\n",
       "0        1.627668e+09                          None                   None   \n",
       "1        1.609642e+09                          None                   None   \n",
       "2                 NaN                                                 None   \n",
       "3        1.654004e+09                          None                   None   \n",
       "4                 NaN                                                 None   \n",
       "\n",
       "  author_flair_richtext author_flair_template_id author_flair_text  ...  \\\n",
       "0                    []                     None              None  ...   \n",
       "1                    []                     None              None  ...   \n",
       "2                   NaN                     None              None  ...   \n",
       "3                    []                     None              None  ...   \n",
       "4                   NaN                     None              None  ...   \n",
       "\n",
       "  total_awards_received treatment_tags upvote_ratio  \\\n",
       "0                     0             []         0.71   \n",
       "1                     0             []         1.00   \n",
       "2                     0             []         0.77   \n",
       "3                     0             []         0.90   \n",
       "4                     0             []         1.00   \n",
       "\n",
       "                                                                                                         url  \\\n",
       "0                     https://www.reddit.com/r/AmItheAsshole/comments/v2fbg0/wibta_if_i_get_my_hair_braided/   \n",
       "1     https://www.reddit.com/r/AmItheAsshole/comments/v2fdaf/aita_for_uninviting_a_best_friend_to_my_gender/   \n",
       "2  https://www.reddit.com/r/AmItheAsshole/comments/v2fdq0/aita_for_being_mad_at_my_friends_for_not_sticking/   \n",
       "3  https://www.reddit.com/r/AmItheAsshole/comments/v2fgt3/aita_for_never_wanting_to_see_a_guy_after_he_lied/   \n",
       "4  https://www.reddit.com/r/AmItheAsshole/comments/v2fh5n/aita_for_not_wanting_my_boyfriends_sister_to_live/   \n",
       "\n",
       "  view_count whitelist_status wls                link_flair_template_id  \\\n",
       "0       None          all_ads   6                                   NaN   \n",
       "1       None          all_ads   6                                   NaN   \n",
       "2       None          all_ads   6  20701dd2-d245-11e8-99f1-0e2d925c15f4   \n",
       "3       None          all_ads   6                                   NaN   \n",
       "4       None          all_ads   6                                   NaN   \n",
       "\n",
       "   call_to_action author_cakeday  \n",
       "0             NaN            NaN  \n",
       "1             NaN            NaN  \n",
       "2             NaN            NaN  \n",
       "3             NaN            NaN  \n",
       "4             NaN            NaN  \n",
       "\n",
       "[5 rows x 92 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>archived</th>\n",
       "      <th>associated_award</th>\n",
       "      <th>author</th>\n",
       "      <th>author_created_utc</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>...</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>top_awarded_type</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>treatment_tags</th>\n",
       "      <th>unrepliable_reason</th>\n",
       "      <th>author_cakeday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>Apewash</td>\n",
       "      <td>1.639883e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>t5_2xhvq</td>\n",
       "      <td>r/AmItheAsshole</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>Adventurous_House527</td>\n",
       "      <td>1.623161e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>t5_2xhvq</td>\n",
       "      <td>r/AmItheAsshole</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>chunkytapioca</td>\n",
       "      <td>1.649170e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>t5_2xhvq</td>\n",
       "      <td>r/AmItheAsshole</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>turtles_tszx</td>\n",
       "      <td>1.503064e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>t5_2xhvq</td>\n",
       "      <td>r/AmItheAsshole</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>Boobear7676</td>\n",
       "      <td>1.653867e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>t5_2xhvq</td>\n",
       "      <td>r/AmItheAsshole</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  archived associated_award                author  \\\n",
       "0            []     False             None               Apewash   \n",
       "1            []     False             None  Adventurous_House527   \n",
       "2            []     False             None         chunkytapioca   \n",
       "3            []     False             None          turtles_tszx   \n",
       "4            []     False             None           Boobear7676   \n",
       "\n",
       "   author_created_utc author_flair_background_color author_flair_css_class  \\\n",
       "0        1.639883e+09                          None                   None   \n",
       "1        1.623161e+09                          None                   None   \n",
       "2        1.649170e+09                          None                   None   \n",
       "3        1.503064e+09                          None                   None   \n",
       "4        1.653867e+09                          None                   None   \n",
       "\n",
       "  author_flair_richtext author_flair_template_id author_flair_text  ...  \\\n",
       "0                    []                     None              None  ...   \n",
       "1                    []                     None              None  ...   \n",
       "2                    []                     None              None  ...   \n",
       "3                    []                     None              None  ...   \n",
       "4                    []                     None              None  ...   \n",
       "\n",
       "  stickied      subreddit subreddit_id subreddit_name_prefixed subreddit_type  \\\n",
       "0    False  AmItheAsshole     t5_2xhvq         r/AmItheAsshole         public   \n",
       "1    False  AmItheAsshole     t5_2xhvq         r/AmItheAsshole         public   \n",
       "2    False  AmItheAsshole     t5_2xhvq         r/AmItheAsshole         public   \n",
       "3    False  AmItheAsshole     t5_2xhvq         r/AmItheAsshole         public   \n",
       "4    False  AmItheAsshole     t5_2xhvq         r/AmItheAsshole         public   \n",
       "\n",
       "  top_awarded_type  total_awards_received  treatment_tags unrepliable_reason  \\\n",
       "0             None                      0              []               None   \n",
       "1             None                      0              []               None   \n",
       "2             None                      0              []               None   \n",
       "3             None                      0              []               None   \n",
       "4             None                      0              []               None   \n",
       "\n",
       "  author_cakeday  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# why: you want to see the schema before committing to transforms\n",
    "df_sub_peek = peek_ndjson(SUBMISSIONS_FILE, PEEK_N)\n",
    "df_com_peek = peek_ndjson(COMMENTS_FILE, PEEK_N)\n",
    "\n",
    "print('submissions columns:', sorted(df_sub_peek.columns.tolist()))\n",
    "print('comments columns:', sorted(df_com_peek.columns.tolist()))\n",
    "\n",
    "display(df_sub_peek.head(5))\n",
    "display(df_com_peek.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e767b154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists: interim\\submissions_minimal.parquet\n"
     ]
    }
   ],
   "source": [
    "# why: shrink submissions to only the fields we need and store them in a fast columnar format for repeated joins/reads\n",
    "\n",
    "SUB_FIELDS = ['id', 'title', 'selftext', 'link_flair_text', 'created_utc']\n",
    "\n",
    "def stream_submissions_to_parquet(src: Path, dst: Path, fields=SUB_FIELDS, batch_size: int = 100_000):\n",
    "    writer = None\n",
    "    rows = []\n",
    "    with src.open('r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            rows.append({k: obj.get(k) for k in fields})\n",
    "            if len(rows) >= batch_size:\n",
    "                df = pd.DataFrame(rows)\n",
    "                table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "                if writer is None:\n",
    "                    writer = pq.ParquetWriter(dst, table.schema)\n",
    "                writer.write_table(table)\n",
    "                rows.clear()\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows)\n",
    "            table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(dst, table.schema)\n",
    "            writer.write_table(table)\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "if not SUBMISSIONS_PARQUET.exists():\n",
    "    SUBMISSIONS_PARQUET.parent.mkdir(parents=True, exist_ok=True)\n",
    "    stream_submissions_to_parquet(SUBMISSIONS_FILE, SUBMISSIONS_PARQUET)\n",
    "    print('wrote:', SUBMISSIONS_PARQUET)\n",
    "else:\n",
    "    print('exists:', SUBMISSIONS_PARQUET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eae14423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submissions frame: (320671, 4) ~290.0 MB in RAM\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>v2fbg0</th>\n",
       "      <td>WIBTA if I get my hair braided</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>None</td>\n",
       "      <td>1654084822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v2fdaf</th>\n",
       "      <td>AITA for uninviting a “best friend” to my gender reveal/housewarming party?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>None</td>\n",
       "      <td>1654084958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v2fdq0</th>\n",
       "      <td>AITA for being mad at my friends for not sticking to our agreements?</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Not enough info</td>\n",
       "      <td>1654084986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              title  \\\n",
       "id                                                                                    \n",
       "v2fbg0                                               WIBTA if I get my hair braided   \n",
       "v2fdaf  AITA for uninviting a “best friend” to my gender reveal/housewarming party?   \n",
       "v2fdq0         AITA for being mad at my friends for not sticking to our agreements?   \n",
       "\n",
       "         selftext  link_flair_text  created_utc  \n",
       "id                                               \n",
       "v2fbg0  [removed]             None   1654084822  \n",
       "v2fdaf  [removed]             None   1654084958  \n",
       "v2fdq0  [deleted]  Not enough info   1654084986  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# why: keep a compact in-RAM table for fast chunk joins; 900 MB NDJSON → much smaller Parquet subset\n",
    "sub_df = pd.read_parquet(SUBMISSIONS_PARQUET, columns=['id', 'title', 'selftext', 'link_flair_text', 'created_utc'])\n",
    "sub_df = sub_df.dropna(subset=['id']).drop_duplicates(subset=['id'])\n",
    "sub_df = sub_df.set_index('id', drop=True)\n",
    "approx_mb = sub_df.memory_usage(deep=True).sum() / 1e6\n",
    "print('submissions frame:', sub_df.shape, f'~{approx_mb:.1f} MB in RAM')\n",
    "display(sub_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28c6b19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using existing joined parts (29 files) in: interim\\joined_parquet\n"
     ]
    }
   ],
   "source": [
    "# why: join comments to their parent submissions without loading all comments into RAM; write results incrementally\n",
    "JOINED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def stream_join_comments(\n",
    "    comments_path: Path,\n",
    "    sub_index_df: pd.DataFrame,\n",
    "    out_dir: Path,\n",
    "    chunk_lines: int = 500_000,\n",
    "    out_prefix: str = 'joined'\n",
    "):\n",
    "    buf = []\n",
    "    file_idx = 0\n",
    "\n",
    "    with comments_path.open('r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                c = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            sub_id = normalize_submission_id_from_link_id(c.get('link_id', ''))\n",
    "            if not sub_id or sub_id not in sub_index_df.index:\n",
    "                continue\n",
    "\n",
    "            s = sub_index_df.loc[sub_id]\n",
    "            buf.append({\n",
    "                'submission_id': sub_id,\n",
    "                'submission_title': s.get('title'),\n",
    "                'submission_selftext': s.get('selftext'),\n",
    "                'submission_flair': s.get('link_flair_text'),\n",
    "                'submission_created_utc': s.get('created_utc'),\n",
    "                'comment_id': c.get('id'),\n",
    "                'comment_body': c.get('body'),\n",
    "                'comment_created_utc': c.get('created_utc'),\n",
    "                'comment_score': c.get('score'),\n",
    "                'is_submitter': c.get('is_submitter'),\n",
    "            })\n",
    "\n",
    "            if len(buf) >= chunk_lines:\n",
    "                df = pd.DataFrame(buf)\n",
    "                pq.write_table(pa.Table.from_pandas(df, preserve_index=False),\n",
    "                               out_dir / f'{out_prefix}_{file_idx:04d}.parquet')\n",
    "                file_idx += 1\n",
    "                buf.clear()\n",
    "\n",
    "    if buf:\n",
    "        df = pd.DataFrame(buf)\n",
    "        pq.write_table(pa.Table.from_pandas(df, preserve_index=False),\n",
    "                       out_dir / f'{out_prefix}_{file_idx:04d}.parquet')\n",
    "\n",
    "\n",
    "# stream_join_comments(COMMENTS_FILE, sub_df, JOINED_DIR, chunk_lines=500_000)\n",
    "# print('wrote joined parts in:', JOINED_DIR)\n",
    "\n",
    "\n",
    "existing_parts = sorted(glob.glob(str(JOINED_DIR / 'joined_*.parquet')))\n",
    "\n",
    "if remake_datafile or not existing_parts:\n",
    "    # optional: clean old parts if forcing a rebuild\n",
    "    if remake_datafile:\n",
    "        for p in existing_parts:\n",
    "            try:\n",
    "                Path(p).unlink()\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "    stream_join_comments(COMMENTS_FILE, sub_df, JOINED_DIR, chunk_lines=500_000) # adjust chunk_lines to available RAM\n",
    "    print('wrote joined parts in:', JOINED_DIR)\n",
    "else:\n",
    "    print(f'using existing joined parts ({len(existing_parts)} files) in:', JOINED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "485084f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joined shape: (14492290, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_title</th>\n",
       "      <th>submission_selftext</th>\n",
       "      <th>submission_flair</th>\n",
       "      <th>submission_created_utc</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>comment_created_utc</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>is_submitter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v2fbg0</td>\n",
       "      <td>WIBTA if I get my hair braided</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>None</td>\n",
       "      <td>1654084822</td>\n",
       "      <td>iaryyw3</td>\n",
       "      <td>^^^^AUTOMOD  ***Thanks for posting! This comment is a copy of your post so readers can see the original text if your post is edited or removed. This comment is NOT accusing you of copying anything. Read [this](https://www.reddit.com/r/AmItheAsshole/wiki/faq#wiki_post_deletion) before [contacting...</td>\n",
       "      <td>1654084822</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v2fdaf</td>\n",
       "      <td>AITA for uninviting a “best friend” to my gender reveal/housewarming party?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>None</td>\n",
       "      <td>1654084958</td>\n",
       "      <td>iarz6sa</td>\n",
       "      <td>#READ THIS CAREFULLY BECAUSE WE WILL PERMANENTLY BAN YOU FOR VIOLATIONS\\n\\n\\nYour post was removed because it exceeds the 3,000 character limit.\\n\\nPlease consider resubmitting a briefer post. You are **not allowed** to continue your post in the comments or another thread. **You will need to pos...</td>\n",
       "      <td>1654084958</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v2fdq0</td>\n",
       "      <td>AITA for being mad at my friends for not sticking to our agreements?</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Not enough info</td>\n",
       "      <td>1654084986</td>\n",
       "      <td>iarz8fs</td>\n",
       "      <td>^^^^AUTOMOD  ***Thanks for posting! This comment is a copy of your post so readers can see the original text if your post is edited or removed. This comment is NOT accusing you of copying anything. Read [this](https://www.reddit.com/r/AmItheAsshole/wiki/faq#wiki_post_deletion) before [contacting...</td>\n",
       "      <td>1654084987</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  submission_id  \\\n",
       "0        v2fbg0   \n",
       "1        v2fdaf   \n",
       "2        v2fdq0   \n",
       "\n",
       "                                                              submission_title  \\\n",
       "0                                               WIBTA if I get my hair braided   \n",
       "1  AITA for uninviting a “best friend” to my gender reveal/housewarming party?   \n",
       "2         AITA for being mad at my friends for not sticking to our agreements?   \n",
       "\n",
       "  submission_selftext submission_flair  submission_created_utc comment_id  \\\n",
       "0           [removed]             None              1654084822    iaryyw3   \n",
       "1           [removed]             None              1654084958    iarz6sa   \n",
       "2           [deleted]  Not enough info              1654084986    iarz8fs   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                  comment_body  \\\n",
       "0  ^^^^AUTOMOD  ***Thanks for posting! This comment is a copy of your post so readers can see the original text if your post is edited or removed. This comment is NOT accusing you of copying anything. Read [this](https://www.reddit.com/r/AmItheAsshole/wiki/faq#wiki_post_deletion) before [contacting...   \n",
       "1  #READ THIS CAREFULLY BECAUSE WE WILL PERMANENTLY BAN YOU FOR VIOLATIONS\\n\\n\\nYour post was removed because it exceeds the 3,000 character limit.\\n\\nPlease consider resubmitting a briefer post. You are **not allowed** to continue your post in the comments or another thread. **You will need to pos...   \n",
       "2  ^^^^AUTOMOD  ***Thanks for posting! This comment is a copy of your post so readers can see the original text if your post is edited or removed. This comment is NOT accusing you of copying anything. Read [this](https://www.reddit.com/r/AmItheAsshole/wiki/faq#wiki_post_deletion) before [contacting...   \n",
       "\n",
       "   comment_created_utc  comment_score  is_submitter  \n",
       "0           1654084822              1         False  \n",
       "1           1654084958              1         False  \n",
       "2           1654084987              1         False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# why: fuse joined parts for downstream analysis; for very large outputs, you can process per-part to keep RAM low\n",
    "import glob\n",
    "\n",
    "joined_parts = sorted(glob.glob(str(JOINED_DIR / 'joined_*.parquet')))\n",
    "if not joined_parts:\n",
    "    raise RuntimeError('no joined parquet parts found. rerun the previous cell.')\n",
    "\n",
    "# if the number of parts is large or your RAM is tight, iterate parts instead of concatenating\n",
    "joined_df = pd.concat([pd.read_parquet(p) for p in joined_parts], ignore_index=True)\n",
    "print('joined shape:', joined_df.shape)\n",
    "display(joined_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fde5ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why: good cleaning + normalisation improves topic modelling and later mapping to themes\n",
    "\n",
    "def ensure_spacy(nlp_name: str = 'en_core_web_sm'):\n",
    "    try:\n",
    "        return spacy.load(nlp_name, disable=['parser', 'textcat'])\n",
    "    except OSError as e:\n",
    "        raise RuntimeError(\n",
    "            f'spaCy model {nlp_name!r} not installed. run: python -m spacy download {nlp_name}'\n",
    "        ) from e\n",
    "\n",
    "NLP = ensure_spacy()\n",
    "\n",
    "URL_RE = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "NONWORD_RE = re.compile(r\"[^a-zA-Z']+\")\n",
    "MULTISPACE_RE = re.compile(r'\\s+')\n",
    "\n",
    "def clean_text(text: Optional[str]) -> str:\n",
    "    if not text:\n",
    "        return ''\n",
    "    text = URL_RE.sub(' ', text)\n",
    "    text = text.lower()\n",
    "    text = NONWORD_RE.sub(' ', text)\n",
    "    text = MULTISPACE_RE.sub(' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def lemmatize(text: str, nlp=NLP, do_ner: bool = False) -> Tuple[str, List[str]]:\n",
    "    if not text:\n",
    "        return '', []\n",
    "    doc = nlp(text)\n",
    "    lemmas = [t.lemma_ for t in doc if not (t.is_stop or t.is_punct or t.is_space)]\n",
    "    ents = [f'{ent.label_}:{ent.text}' for ent in doc.ents] if do_ner else []\n",
    "    return ' '.join(lemmas), ents\n",
    "\n",
    "def preprocess_submission_row(row: dict, do_ner: bool = True) -> dict:\n",
    "    raw = ' '.join([str(row.get('title') or ''), str(row.get('selftext') or '')]).strip()\n",
    "    cleaned = clean_text(raw)\n",
    "    lemmas, ents = lemmatize(cleaned, do_ner=do_ner)\n",
    "    return {\n",
    "        'id': row.get('id'),\n",
    "        'flair': row.get('link_flair_text'),\n",
    "        'created_utc': row.get('created_utc'),\n",
    "        'text_raw': raw,\n",
    "        'text_clean': cleaned,\n",
    "        'text_lemmas': lemmas,\n",
    "        'ents': ents\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c9b9413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total submissions: 320671\n",
      "With usable body: 68011\n",
      "Flair available: 79223\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_len</th>\n",
       "      <th>body_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>320671.000000</td>\n",
       "      <td>320671.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>61.227096</td>\n",
       "      <td>420.817483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>25.048349</td>\n",
       "      <td>883.220962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>44.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>57.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>74.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>311.000000</td>\n",
       "      <td>23331.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title_len       body_len\n",
       "count  320671.000000  320671.000000\n",
       "mean       61.227096     420.817483\n",
       "std        25.048349     883.220962\n",
       "min        20.000000       0.000000\n",
       "25%        44.000000       9.000000\n",
       "50%        57.000000       9.000000\n",
       "75%        74.000000       9.000000\n",
       "max       311.000000   23331.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable submissions: 275324\n",
      "Saved: interim\\submissions_usable.parquet\n"
     ]
    }
   ],
   "source": [
    "# why: many posts have selftext '[removed]'/'[deleted]' or empty; topic modelling needs actual text\n",
    "\n",
    "REMOVED_MARKERS = {'[removed]', '[deleted]', None, ''}\n",
    "\n",
    "def is_removed(txt):\n",
    "    return (txt is None) or (str(txt).strip() in REMOVED_MARKERS)\n",
    "\n",
    "def text_len(s):\n",
    "    return 0 if s is None else len(str(s).strip())\n",
    "\n",
    "# basic diagnostics on your submissions index frame\n",
    "diag = pd.DataFrame({\n",
    "    'has_body': ~sub_df['selftext'].apply(is_removed),\n",
    "    'title_len': sub_df['title'].apply(text_len),\n",
    "    'body_len': sub_df['selftext'].apply(text_len),\n",
    "    'flair_none': sub_df['link_flair_text'].isna()\n",
    "})\n",
    "print('Total submissions:', len(sub_df))\n",
    "print('With usable body:', int(diag['has_body'].sum()))\n",
    "print('Flair available:', int((~diag[\"flair_none\"]).sum()))\n",
    "display(diag.describe())\n",
    "\n",
    "# build a \"usable\" subset:\n",
    "# keep if (body is usable and body_len >= 50) OR (title_len >= 40)\n",
    "usable_mask = (~sub_df['selftext'].apply(is_removed) & (sub_df['selftext'].apply(text_len) >= 50)) | (sub_df['title'].apply(text_len) >= 40)\n",
    "usable_sub_df = sub_df.loc[usable_mask, ['title','selftext','link_flair_text','created_utc']].copy()\n",
    "print('Usable submissions:', len(usable_sub_df))\n",
    "\n",
    "# for repeatable sampling\n",
    "USABLE_SUBMISSIONS_PARQUET = INTERIM_DIR / 'submissions_usable.parquet'\n",
    "usable_sub_df.to_parquet(USABLE_SUBMISSIONS_PARQUET, index=True)\n",
    "print('Saved:', USABLE_SUBMISSIONS_PARQUET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d63e4ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submissions sampled for modelling (usable only): 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yiud1m</td>\n",
       "      <td>AITA for carding everyone in a party coming into a liquor store?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>None</td>\n",
       "      <td>1667264304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yvcwyh</td>\n",
       "      <td>AITA for calling out my friend for ditching me for tennis then playing with other people instead</td>\n",
       "      <td>Throwaway account here. \\n\\nI (26m) was supposed to play tennis with one of my best friends \"Drew\" (26m) and his brother \"Ivan\"(29m) a few weekends ago. On the morning of, Drew sent a message saying he was too hungover to come down (20-25 minute drive), so the three of us made plans to play the ...</td>\n",
       "      <td>Not the A-hole</td>\n",
       "      <td>1668460623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x3iyei</td>\n",
       "      <td>AITA for not sleeping on the couch?</td>\n",
       "      <td>My GF and I had our first ever major argument last night and she basically told me to gtfo from our room and go sleep on the couch. I refused which ultimately lead to a screaming match between the two of us, ending up in her leaving our apartment and spending the night at one of her friend's ins...</td>\n",
       "      <td>Not the A-hole</td>\n",
       "      <td>1662067240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  \\\n",
       "0  yiud1m   \n",
       "1  yvcwyh   \n",
       "2  x3iyei   \n",
       "\n",
       "                                                                                              title  \\\n",
       "0                                  AITA for carding everyone in a party coming into a liquor store?   \n",
       "1  AITA for calling out my friend for ditching me for tennis then playing with other people instead   \n",
       "2                                                               AITA for not sleeping on the couch?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      selftext  \\\n",
       "0                                                                                                                                                                                                                                                                                                    [removed]   \n",
       "1  Throwaway account here. \\n\\nI (26m) was supposed to play tennis with one of my best friends \"Drew\" (26m) and his brother \"Ivan\"(29m) a few weekends ago. On the morning of, Drew sent a message saying he was too hungover to come down (20-25 minute drive), so the three of us made plans to play the ...   \n",
       "2  My GF and I had our first ever major argument last night and she basically told me to gtfo from our room and go sleep on the couch. I refused which ultimately lead to a screaming match between the two of us, ending up in her leaving our apartment and spending the night at one of her friend's ins...   \n",
       "\n",
       "  link_flair_text  created_utc  \n",
       "0            None   1667264304  \n",
       "1  Not the A-hole   1668460623  \n",
       "2  Not the A-hole   1662067240  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>flair</th>\n",
       "      <th>text_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yiud1m</td>\n",
       "      <td>None</td>\n",
       "      <td>AITA for carding everyone in a party coming into a liquor store? [removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yvcwyh</td>\n",
       "      <td>Not the A-hole</td>\n",
       "      <td>AITA for calling out my friend for ditching me for tennis then playing with other people instead Throwaway account here. \\n\\nI (26m) was supposed to play tennis with one of my best friends \"Drew\" (26m) and his brother \"Ivan\"(29m) a few weekends ago. On the morning of, Drew sent a message saying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x3iyei</td>\n",
       "      <td>Not the A-hole</td>\n",
       "      <td>AITA for not sleeping on the couch? My GF and I had our first ever major argument last night and she basically told me to gtfo from our room and go sleep on the couch. I refused which ultimately lead to a screaming match between the two of us, ending up in her leaving our apartment and spending ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id           flair  \\\n",
       "0  yiud1m            None   \n",
       "1  yvcwyh  Not the A-hole   \n",
       "2  x3iyei  Not the A-hole   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      text_raw  \n",
       "0                                                                                                                                                                                                                                   AITA for carding everyone in a party coming into a liquor store? [removed]  \n",
       "1  AITA for calling out my friend for ditching me for tennis then playing with other people instead Throwaway account here. \\n\\nI (26m) was supposed to play tennis with one of my best friends \"Drew\" (26m) and his brother \"Ivan\"(29m) a few weekends ago. On the morning of, Drew sent a message saying ...  \n",
       "2  AITA for not sleeping on the couch? My GF and I had our first ever major argument last night and she basically told me to gtfo from our room and go sleep on the couch. I refused which ultimately lead to a screaming match between the two of us, ending up in her leaving our apartment and spending ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# why: sample from text that’s actually usable, so LDA/NER have signal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_random_submissions_df_from_usable(usable_index_df: pd.DataFrame, n: int, seed: int = 42) -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    ids = usable_index_df.index.values\n",
    "    pick = ids if n >= len(ids) else rng.choice(ids, size=n, replace=False)\n",
    "    df = usable_index_df.loc[pick, ['title', 'selftext', 'link_flair_text', 'created_utc']].reset_index()\n",
    "    df = df.rename(columns={'index': 'id'})\n",
    "    return df\n",
    "\n",
    "def build_corpus(df_sub: pd.DataFrame, do_ner: bool = True) -> pd.DataFrame:\n",
    "    processed = [preprocess_submission_row(row, do_ner=do_ner) for row in df_sub.to_dict('records')]\n",
    "    return pd.DataFrame(processed)\n",
    "\n",
    "# IMPORTANT: sample from usable_sub_df, not sub_df\n",
    "df_sub_sample = fetch_random_submissions_df_from_usable(usable_sub_df, SUBMISSION_SAMPLE_N, seed=RANDOM_SEED)\n",
    "print('Submissions sampled for modelling (usable only):', len(df_sub_sample))\n",
    "display(df_sub_sample.head(3))\n",
    "\n",
    "df_corpus = build_corpus(df_sub_sample, do_ner=True)\n",
    "display(df_corpus.head(3)[['id', 'flair', 'text_raw']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c26be2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topics_top_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m best_cat, scores\n\u001b[32m     29\u001b[39m topic_category = []\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, words \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtopics_top_words\u001b[49m):\n\u001b[32m     31\u001b[39m     best, scores = score_topic_to_category(words)\n\u001b[32m     32\u001b[39m     topic_category.append({\u001b[33m'\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m'\u001b[39m: i, \u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m: best, **scores})\n",
      "\u001b[31mNameError\u001b[39m: name 'topics_top_words' is not defined"
     ]
    }
   ],
   "source": [
    "# why: RQ2 needs concrete buckets; we map discovered topics to five categories using keyword overlaps\n",
    "\n",
    "CATEGORY_KEYWORDS = {\n",
    "    'finances': {\n",
    "        'money','pay','paid','rent','bill','bills','loan','debt','card','credit','cash','salary','bonus','split','cost','expensive','cheap','wedding','gift','refund','share','finance'\n",
    "    },\n",
    "    'relationship': {\n",
    "        'relationship','boyfriend','girlfriend','partner','date','dating','romantic','love','cheat','ex','fiancé','fiance','fiancee','breakup','trust','jealous'\n",
    "    },\n",
    "    'family_conflict': {\n",
    "        'mom','dad','mother','father','sister','brother','siblings','family','cousin','aunt','uncle','inlaws','in','law','grandma','grandpa','child','baby','pregnant','wedding','name'\n",
    "    },\n",
    "    'work': {\n",
    "        'work','job','boss','coworker','manager','shift','hours','office','remote','payroll','promotion','hr','fire','fired','leave','paternity','maternity'\n",
    "    },\n",
    "    'societal_norms': {\n",
    "        'culture','religion','religious','tradition','gender','pronoun','politics','law','legal','illegal','discrimination','racist','ableist','ethics','value','norm','boundary','consent'\n",
    "    }\n",
    "}\n",
    "\n",
    "def score_topic_to_category(words: List[str]) -> Tuple[str, Dict[str, int]]:\n",
    "    scores = {cat: 0 for cat in CATEGORY_KEYWORDS}\n",
    "    wordset = set(words)\n",
    "    for cat, kw in CATEGORY_KEYWORDS.items():\n",
    "        scores[cat] = len(wordset & kw)\n",
    "    best_cat = max(scores, key=scores.get)\n",
    "    return best_cat, scores\n",
    "\n",
    "topic_category = []\n",
    "for i, words in enumerate(topics_top_words):\n",
    "    best, scores = score_topic_to_category(words)\n",
    "    topic_category.append({'topic': i, 'category': best, **scores})\n",
    "\n",
    "df_topic_map = pd.DataFrame(topic_category).sort_values(['category', 'topic'])\n",
    "display(df_topic_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6691778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why: per-submission labels allow counts, examples, and trends to answer RQ2 precisely\n",
    "\n",
    "topic_labels = np.argmax(W, axis=1)\n",
    "df_corpus['topic'] = topic_labels\n",
    "\n",
    "topic_to_cat = {row['topic']: row['category'] for _, row in df_topic_map.iterrows()}\n",
    "df_corpus['category_initial'] = df_corpus['topic'].map(topic_to_cat).fillna('societal_norms')\n",
    "\n",
    "def ner_bias_category(ents: List[str], current: str) -> str:\n",
    "    labels = [e.split(':', 1)[0] for e in ents]\n",
    "    if any(lbl in ('NORP', 'LAW') for lbl in labels) and current in ('relationship', 'work', 'finances'):\n",
    "        return 'societal_norms'\n",
    "    if any(lbl in ('PERSON',) for lbl in labels) and current == 'societal_norms':\n",
    "        return 'family_conflict'\n",
    "    return current\n",
    "\n",
    "df_corpus['category'] = [\n",
    "    ner_bias_category(ents, cat) for ents, cat in zip(df_corpus['ents'], df_corpus['category_initial'])\n",
    "]\n",
    "\n",
    "category_counts = df_corpus['category'].value_counts().rename_axis('category').reset_index(name='count')\n",
    "display(category_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e71037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why: quick human check to see if buckets make sense before scaling up\n",
    "\n",
    "def examples_by_category(df: pd.DataFrame, cat: str, k: int = 5) -> pd.DataFrame:\n",
    "    ex = df.loc[df['category'] == cat, ['id', 'flair', 'text_raw']].head(k).copy()\n",
    "    return ex\n",
    "\n",
    "for cat in ['finances', 'relationship', 'family_conflict', 'work', 'societal_norms']:\n",
    "    print(f'\\n=== {cat.upper()} EXAMPLES ===')\n",
    "    display(examples_by_category(df_corpus, cat, k=5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ti3160tu-nlp-group-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
