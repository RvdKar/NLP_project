{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f57099",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df7e8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Iterable, Dict, List, Tuple, Optional\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# display prefs\n",
    "pd.set_option('display.max_colwidth', 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb5629",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fa72689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "data_dir = Path('data')\n",
    "comments_file = data_dir / 'amitheasshole_comments.ndjson'\n",
    "submissions_file = data_dir / 'amitheasshole_submissions.ndjson'\n",
    "\n",
    "remake_datafile = False  # set True to force rebuilds\n",
    "\n",
    "interim_dir = Path('data/parquet')\n",
    "artifacts_dir = Path('artifacts')\n",
    "for p in [interim_dir, artifacts_dir]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# parquet outputs\n",
    "submissions_parquet = interim_dir / 'submissions_minimal.parquet'\n",
    "joined_dir = interim_dir / 'joined_parquet'\n",
    "\n",
    "# peek settings\n",
    "peek_n = 1_000\n",
    "\n",
    "# modelling sample size\n",
    "submission_sample_n = 1_000\n",
    "random_seed = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a5006",
   "metadata": {},
   "source": [
    "## ndjson loading and displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad38763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the comments file is ~22 GB; never load fully into RAM. stream lines lazily.\n",
    "\n",
    "def iter_ndjson_lines(path: Path) -> Iterator[dict]:\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # corrupted line guard; skip\n",
    "                continue\n",
    "\n",
    "def peek_ndjson(path: Path, n: int) -> pd.DataFrame:\n",
    "    # small materialisation to learn schema\n",
    "    rows = []\n",
    "    for i, obj in enumerate(iter_ndjson_lines(path)):\n",
    "        rows.append(obj)\n",
    "        if i + 1 >= n:\n",
    "            break\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def normalize_submission_id_from_link_id(link_id: str) -> Optional[str]:\n",
    "    # comments store parent submission as 't3_<id>'; we need bare '<id>'\n",
    "    if not link_id:\n",
    "        return None\n",
    "    parts = link_id.split('_', 1)\n",
    "    return parts[1] if len(parts) == 2 else link_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc38151a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submissions columns: ['all_awardings', 'allow_live_comments', 'archived', 'author', 'author_cakeday', 'author_created_utc', 'author_flair_background_color', 'author_flair_css_class', 'author_flair_richtext', 'author_flair_template_id', 'author_flair_text', 'author_flair_text_color', 'author_flair_type', 'author_fullname', 'author_patreon_flair', 'author_premium', 'awarders', 'banned_by', 'call_to_action', 'can_gild', 'can_mod_post', 'category', 'content_categories', 'contest_mode', 'created_utc', 'discussion_type', 'distinguished', 'domain', 'edited', 'gilded', 'gildings', 'hidden', 'hide_score', 'id', 'is_created_from_ads_ui', 'is_crosspostable', 'is_meta', 'is_original_content', 'is_reddit_media_domain', 'is_robot_indexable', 'is_self', 'is_video', 'link_flair_background_color', 'link_flair_css_class', 'link_flair_richtext', 'link_flair_template_id', 'link_flair_text', 'link_flair_text_color', 'link_flair_type', 'locked', 'media', 'media_embed', 'media_only', 'name', 'no_follow', 'num_comments', 'num_crossposts', 'over_18', 'parent_whitelist_status', 'permalink', 'pinned', 'pwls', 'quarantine', 'removed_by', 'removed_by_category', 'retrieved_on', 'retrieved_utc', 'score', 'secure_media', 'secure_media_embed', 'selftext', 'send_replies', 'spoiler', 'stickied', 'subreddit', 'subreddit_id', 'subreddit_name_prefixed', 'subreddit_subscribers', 'subreddit_type', 'suggested_sort', 'thumbnail', 'thumbnail_height', 'thumbnail_width', 'title', 'top_awarded_type', 'total_awards_received', 'treatment_tags', 'upvote_ratio', 'url', 'view_count', 'whitelist_status', 'wls']\n",
      "comments columns: ['all_awardings', 'archived', 'associated_award', 'author', 'author_cakeday', 'author_created_utc', 'author_flair_background_color', 'author_flair_css_class', 'author_flair_richtext', 'author_flair_template_id', 'author_flair_text', 'author_flair_text_color', 'author_flair_type', 'author_fullname', 'author_patreon_flair', 'author_premium', 'body', 'can_gild', 'collapsed', 'collapsed_because_crowd_control', 'collapsed_reason', 'collapsed_reason_code', 'comment_type', 'controversiality', 'created_utc', 'distinguished', 'edited', 'gilded', 'gildings', 'id', 'is_submitter', 'link_id', 'locked', 'name', 'no_follow', 'parent_id', 'permalink', 'retrieved_on', 'score', 'score_hidden', 'send_replies', 'stickied', 'subreddit', 'subreddit_id', 'subreddit_name_prefixed', 'subreddit_type', 'top_awarded_type', 'total_awards_received', 'treatment_tags', 'unrepliable_reason']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_created_utc</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>...</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>treatment_tags</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>url</th>\n",
       "      <th>view_count</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "      <th>link_flair_template_id</th>\n",
       "      <th>call_to_action</th>\n",
       "      <th>author_cakeday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>beanstressed</td>\n",
       "      <td>1.627668e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.71</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comments/v2fbg0/wibta_if_i_get_my_hair_braided/</td>\n",
       "      <td>None</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Good-Barracuda5143</td>\n",
       "      <td>1.609642e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comments/v2fdaf/aita_for_uninviting_a_best_friend_to_my_gender/</td>\n",
       "      <td>None</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.77</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comments/v2fdq0/aita_for_being_mad_at_my_friends_for_not_sticking/</td>\n",
       "      <td>None</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>20701dd2-d245-11e8-99f1-0e2d925c15f4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>LisKoz1989</td>\n",
       "      <td>1.654004e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.90</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comments/v2fgt3/aita_for_never_wanting_to_see_a_guy_after_he_lied/</td>\n",
       "      <td>None</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.00</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comments/v2fh5n/aita_for_not_wanting_my_boyfriends_sister_to_live/</td>\n",
       "      <td>None</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments  archived              author  \\\n",
       "0            []                False     False        beanstressed   \n",
       "1            []                False     False  Good-Barracuda5143   \n",
       "2            []                 True     False           [deleted]   \n",
       "3            []                False     False          LisKoz1989   \n",
       "4            []                False     False           [deleted]   \n",
       "\n",
       "   author_created_utc author_flair_background_color author_flair_css_class  \\\n",
       "0        1.627668e+09                          None                   None   \n",
       "1        1.609642e+09                          None                   None   \n",
       "2                 NaN                                                 None   \n",
       "3        1.654004e+09                          None                   None   \n",
       "4                 NaN                                                 None   \n",
       "\n",
       "  author_flair_richtext author_flair_template_id author_flair_text  ...  \\\n",
       "0                    []                     None              None  ...   \n",
       "1                    []                     None              None  ...   \n",
       "2                   NaN                     None              None  ...   \n",
       "3                    []                     None              None  ...   \n",
       "4                   NaN                     None              None  ...   \n",
       "\n",
       "  total_awards_received treatment_tags upvote_ratio  \\\n",
       "0                     0             []         0.71   \n",
       "1                     0             []         1.00   \n",
       "2                     0             []         0.77   \n",
       "3                     0             []         0.90   \n",
       "4                     0             []         1.00   \n",
       "\n",
       "                                                                                                         url  \\\n",
       "0                     https://www.reddit.com/r/AmItheAsshole/comments/v2fbg0/wibta_if_i_get_my_hair_braided/   \n",
       "1     https://www.reddit.com/r/AmItheAsshole/comments/v2fdaf/aita_for_uninviting_a_best_friend_to_my_gender/   \n",
       "2  https://www.reddit.com/r/AmItheAsshole/comments/v2fdq0/aita_for_being_mad_at_my_friends_for_not_sticking/   \n",
       "3  https://www.reddit.com/r/AmItheAsshole/comments/v2fgt3/aita_for_never_wanting_to_see_a_guy_after_he_lied/   \n",
       "4  https://www.reddit.com/r/AmItheAsshole/comments/v2fh5n/aita_for_not_wanting_my_boyfriends_sister_to_live/   \n",
       "\n",
       "  view_count whitelist_status wls                link_flair_template_id  \\\n",
       "0       None          all_ads   6                                   NaN   \n",
       "1       None          all_ads   6                                   NaN   \n",
       "2       None          all_ads   6  20701dd2-d245-11e8-99f1-0e2d925c15f4   \n",
       "3       None          all_ads   6                                   NaN   \n",
       "4       None          all_ads   6                                   NaN   \n",
       "\n",
       "   call_to_action author_cakeday  \n",
       "0             NaN            NaN  \n",
       "1             NaN            NaN  \n",
       "2             NaN            NaN  \n",
       "3             NaN            NaN  \n",
       "4             NaN            NaN  \n",
       "\n",
       "[5 rows x 92 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>archived</th>\n",
       "      <th>associated_award</th>\n",
       "      <th>author</th>\n",
       "      <th>author_created_utc</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>...</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>top_awarded_type</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>treatment_tags</th>\n",
       "      <th>unrepliable_reason</th>\n",
       "      <th>author_cakeday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>Apewash</td>\n",
       "      <td>1.639883e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>t5_2xhvq</td>\n",
       "      <td>r/AmItheAsshole</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>Adventurous_House527</td>\n",
       "      <td>1.623161e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>t5_2xhvq</td>\n",
       "      <td>r/AmItheAsshole</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>chunkytapioca</td>\n",
       "      <td>1.649170e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>t5_2xhvq</td>\n",
       "      <td>r/AmItheAsshole</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>turtles_tszx</td>\n",
       "      <td>1.503064e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>t5_2xhvq</td>\n",
       "      <td>r/AmItheAsshole</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>Boobear7676</td>\n",
       "      <td>1.653867e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>t5_2xhvq</td>\n",
       "      <td>r/AmItheAsshole</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  archived associated_award                author  \\\n",
       "0            []     False             None               Apewash   \n",
       "1            []     False             None  Adventurous_House527   \n",
       "2            []     False             None         chunkytapioca   \n",
       "3            []     False             None          turtles_tszx   \n",
       "4            []     False             None           Boobear7676   \n",
       "\n",
       "   author_created_utc author_flair_background_color author_flair_css_class  \\\n",
       "0        1.639883e+09                          None                   None   \n",
       "1        1.623161e+09                          None                   None   \n",
       "2        1.649170e+09                          None                   None   \n",
       "3        1.503064e+09                          None                   None   \n",
       "4        1.653867e+09                          None                   None   \n",
       "\n",
       "  author_flair_richtext author_flair_template_id author_flair_text  ...  \\\n",
       "0                    []                     None              None  ...   \n",
       "1                    []                     None              None  ...   \n",
       "2                    []                     None              None  ...   \n",
       "3                    []                     None              None  ...   \n",
       "4                    []                     None              None  ...   \n",
       "\n",
       "  stickied      subreddit subreddit_id subreddit_name_prefixed subreddit_type  \\\n",
       "0    False  AmItheAsshole     t5_2xhvq         r/AmItheAsshole         public   \n",
       "1    False  AmItheAsshole     t5_2xhvq         r/AmItheAsshole         public   \n",
       "2    False  AmItheAsshole     t5_2xhvq         r/AmItheAsshole         public   \n",
       "3    False  AmItheAsshole     t5_2xhvq         r/AmItheAsshole         public   \n",
       "4    False  AmItheAsshole     t5_2xhvq         r/AmItheAsshole         public   \n",
       "\n",
       "  top_awarded_type  total_awards_received  treatment_tags unrepliable_reason  \\\n",
       "0             None                      0              []               None   \n",
       "1             None                      0              []               None   \n",
       "2             None                      0              []               None   \n",
       "3             None                      0              []               None   \n",
       "4             None                      0              []               None   \n",
       "\n",
       "  author_cakeday  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sub_peek = peek_ndjson(submissions_file, peek_n)\n",
    "df_com_peek = peek_ndjson(comments_file, peek_n)\n",
    "\n",
    "print('submissions columns:', sorted(df_sub_peek.columns.tolist()))\n",
    "print('comments columns:', sorted(df_com_peek.columns.tolist()))\n",
    "\n",
    "display(df_sub_peek.head(5))\n",
    "display(df_com_peek.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4359466",
   "metadata": {},
   "source": [
    "## Load, convert and combine submisions and comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2592ca15",
   "metadata": {},
   "source": [
    "#### submissions to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7478a69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists: data\\parquet\\submissions_minimal.parquet\n"
     ]
    }
   ],
   "source": [
    "# shrink submissions to only the fields we need and store them in a fast format\n",
    "sub_fields = ['id', 'title', 'selftext', 'link_flair_text', 'created_utc']\n",
    "\n",
    "def stream_submissions_to_parquet(src: Path, dst: Path, fields=sub_fields, batch_size: int = 100_000):\n",
    "    writer = None\n",
    "    rows = []\n",
    "    with src.open('r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            rows.append({k: obj.get(k) for k in fields})\n",
    "            if len(rows) >= batch_size:\n",
    "                df = pd.DataFrame(rows)\n",
    "                table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "                if writer is None:\n",
    "                    writer = pq.ParquetWriter(dst, table.schema)\n",
    "                writer.write_table(table)\n",
    "                rows.clear()\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows)\n",
    "            table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(dst, table.schema)\n",
    "            writer.write_table(table)\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "if remake_datafile or not submissions_parquet.exists():\n",
    "    submissions_parquet.parent.mkdir(parents=True, exist_ok=True)\n",
    "    stream_submissions_to_parquet(submissions_file, submissions_parquet)\n",
    "    print('wrote:', submissions_parquet)\n",
    "else:\n",
    "    print('exists:', submissions_parquet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c3422",
   "metadata": {},
   "source": [
    "#### load submissions into ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d04159be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submissions frame: (320671, 4) ~290.0 MB in RAM\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>v2fbg0</th>\n",
       "      <td>WIBTA if I get my hair braided</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>None</td>\n",
       "      <td>1654084822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v2fdaf</th>\n",
       "      <td>AITA for uninviting a “best friend” to my gender reveal/housewarming party?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>None</td>\n",
       "      <td>1654084958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v2fdq0</th>\n",
       "      <td>AITA for being mad at my friends for not sticking to our agreements?</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Not enough info</td>\n",
       "      <td>1654084986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              title  \\\n",
       "id                                                                                    \n",
       "v2fbg0                                               WIBTA if I get my hair braided   \n",
       "v2fdaf  AITA for uninviting a “best friend” to my gender reveal/housewarming party?   \n",
       "v2fdq0         AITA for being mad at my friends for not sticking to our agreements?   \n",
       "\n",
       "         selftext  link_flair_text  created_utc  \n",
       "id                                               \n",
       "v2fbg0  [removed]             None   1654084822  \n",
       "v2fdaf  [removed]             None   1654084958  \n",
       "v2fdq0  [deleted]  Not enough info   1654084986  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub_df = pd.read_parquet(submissions_parquet, columns=['id', 'title', 'selftext', 'link_flair_text', 'created_utc'])\n",
    "sub_df = sub_df.dropna(subset=['id']).drop_duplicates(subset=['id'])\n",
    "sub_df = sub_df.set_index('id', drop=True)\n",
    "approx_mb = sub_df.memory_usage(deep=True).sum() / 1e6\n",
    "print('submissions frame:', sub_df.shape, f'~{approx_mb:.1f} MB in RAM')\n",
    "display(sub_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d404fab2",
   "metadata": {},
   "source": [
    "#### hierarchical comments and submissions parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d324ad07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using existing hierarchical file: data\\parquet\\submissions_hierarchical.parquet (320065 submissions)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_title</th>\n",
       "      <th>submission_selftext</th>\n",
       "      <th>submission_flair</th>\n",
       "      <th>submission_created_utc</th>\n",
       "      <th>comment_ids</th>\n",
       "      <th>comment_bodies</th>\n",
       "      <th>comment_scores</th>\n",
       "      <th>comment_created_utcs</th>\n",
       "      <th>is_submitters</th>\n",
       "      <th>n_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v2fbg0</td>\n",
       "      <td>WIBTA if I get my hair braided</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>None</td>\n",
       "      <td>1.654085e+09</td>\n",
       "      <td>[iaryyw3, iarzk14, iarzybo, ias02ip, ias06pw, ias0cjn, ias104h, ias1rmz, ias1v31, ias1zom, ias2dts, ias3dtt, ias3fox, ias42lm, ias46iy, ias4fuq, ias4jjo, ias51fx, ias5r51, ias64gi, ias6q9e, ias75w1, ias7nt9, ias8ejd, ias9sbq, ias9w0x, iasa5ap, iasaepx, iasaok3, iasaw0i, iasb3y1, iasbsbc, iasca3t...</td>\n",
       "      <td>[^^^^AUTOMOD  ***Thanks for posting! This comment is a copy of your post so readers can see the original text if your post is edited or removed. This comment is NOT accusing you of copying anything. Read [this](https://www.reddit.com/r/AmItheAsshole/wiki/faq#wiki_post_deletion) before [contactin...</td>\n",
       "      <td>[1, 1, -3, 0, -1, 39, 1, 19, 3, 1, -6, 15, 4, 5, 27, 2, -4, 7, -5, 1, 4, 6, 8, 37, 48, 10, 11, -3, 9, 4, -9, 1, 6, -6, 3, 0, 4, 2, 6, 3, -3, 4, 0, 0, 9, 3, -4, 4, 1, 1, 3, 2, 1, 1]</td>\n",
       "      <td>[1654084822.0, 1654085181.0, 1654085425.0, 1654085496.0, 1654085566.0, 1654085664.0, 1654086051.0, 1654086498.0, 1654086553.0, 1654086626.0, 1654086855.0, 1654087414.0, 1654087442.0, 1654087785.0, 1654087843.0, 1654087982.0, 1654088039.0, 1654088308.0, 1654088687.0, 1654088878.0, 1654089190.0, 1...</td>\n",
       "      <td>[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, F...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v2fdaf</td>\n",
       "      <td>AITA for uninviting a “best friend” to my gender reveal/housewarming party?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>None</td>\n",
       "      <td>1.654085e+09</td>\n",
       "      <td>[iarz6sa]</td>\n",
       "      <td>[#READ THIS CAREFULLY BECAUSE WE WILL PERMANENTLY BAN YOU FOR VIOLATIONS\\n\\n\\nYour post was removed because it exceeds the 3,000 character limit.\\n\\nPlease consider resubmitting a briefer post. You are **not allowed** to continue your post in the comments or another thread. **You will need to po...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1654084958.0]</td>\n",
       "      <td>[False]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  submission_id  \\\n",
       "0        v2fbg0   \n",
       "1        v2fdaf   \n",
       "\n",
       "                                                              submission_title  \\\n",
       "0                                               WIBTA if I get my hair braided   \n",
       "1  AITA for uninviting a “best friend” to my gender reveal/housewarming party?   \n",
       "\n",
       "  submission_selftext submission_flair  submission_created_utc  \\\n",
       "0           [removed]             None            1.654085e+09   \n",
       "1           [removed]             None            1.654085e+09   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                   comment_ids  \\\n",
       "0  [iaryyw3, iarzk14, iarzybo, ias02ip, ias06pw, ias0cjn, ias104h, ias1rmz, ias1v31, ias1zom, ias2dts, ias3dtt, ias3fox, ias42lm, ias46iy, ias4fuq, ias4jjo, ias51fx, ias5r51, ias64gi, ias6q9e, ias75w1, ias7nt9, ias8ejd, ias9sbq, ias9w0x, iasa5ap, iasaepx, iasaok3, iasaw0i, iasb3y1, iasbsbc, iasca3t...   \n",
       "1                                                                                                                                                                                                                                                                                                    [iarz6sa]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                comment_bodies  \\\n",
       "0  [^^^^AUTOMOD  ***Thanks for posting! This comment is a copy of your post so readers can see the original text if your post is edited or removed. This comment is NOT accusing you of copying anything. Read [this](https://www.reddit.com/r/AmItheAsshole/wiki/faq#wiki_post_deletion) before [contactin...   \n",
       "1  [#READ THIS CAREFULLY BECAUSE WE WILL PERMANENTLY BAN YOU FOR VIOLATIONS\\n\\n\\nYour post was removed because it exceeds the 3,000 character limit.\\n\\nPlease consider resubmitting a briefer post. You are **not allowed** to continue your post in the comments or another thread. **You will need to po...   \n",
       "\n",
       "                                                                                                                                                                         comment_scores  \\\n",
       "0  [1, 1, -3, 0, -1, 39, 1, 19, 3, 1, -6, 15, 4, 5, 27, 2, -4, 7, -5, 1, 4, 6, 8, 37, 48, 10, 11, -3, 9, 4, -9, 1, 6, -6, 3, 0, 4, 2, 6, 3, -3, 4, 0, 0, 9, 3, -4, 4, 1, 1, 3, 2, 1, 1]   \n",
       "1                                                                                                                                                                                   [1]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                          comment_created_utcs  \\\n",
       "0  [1654084822.0, 1654085181.0, 1654085425.0, 1654085496.0, 1654085566.0, 1654085664.0, 1654086051.0, 1654086498.0, 1654086553.0, 1654086626.0, 1654086855.0, 1654087414.0, 1654087442.0, 1654087785.0, 1654087843.0, 1654087982.0, 1654088039.0, 1654088308.0, 1654088687.0, 1654088878.0, 1654089190.0, 1...   \n",
       "1                                                                                                                                                                                                                                                                                               [1654084958.0]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                 is_submitters  \\\n",
       "0  [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, F...   \n",
       "1                                                                                                                                                                                                                                                                                                      [False]   \n",
       "\n",
       "   n_comments  \n",
       "0          54  \n",
       "1           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build a hierarchical table directly from the raw comments + in-memory submissions index (sub_df)\n",
    "\n",
    "hier_parquet = interim_dir / 'submissions_hierarchical.parquet'\n",
    "\n",
    "# knobs\n",
    "include_op_only = False  # set True to only keep OP (is_submitter == True) comments\n",
    "\n",
    "def read_parquet_nested_safe(path: Path) -> pd.DataFrame:\n",
    "    # why: safely load parquet with list (nested) columns\n",
    "    table = pq.read_table(path)\n",
    "    table = table.combine_chunks()\n",
    "    return table.to_pandas()\n",
    "\n",
    "# type-casting helpers to enforce homogeneous list element types\n",
    "def _to_str(x):\n",
    "    return None if x is None else str(x)\n",
    "\n",
    "def _to_int(x):\n",
    "    if x is None: return None\n",
    "    try: return int(x)\n",
    "    except Exception: return None\n",
    "\n",
    "def _to_float(x):\n",
    "    if x is None: return None\n",
    "    try: return float(x)\n",
    "    except Exception: return None\n",
    "\n",
    "def _to_bool(x):\n",
    "    if x is None: return None\n",
    "    return bool(x)\n",
    "\n",
    "def build_hierarchical_direct(comments_path: Path,\n",
    "                              sub_index_df: pd.DataFrame,\n",
    "                              out_path: Path,\n",
    "                              include_op_only: bool = False,\n",
    "                              limit_lines: Optional[int] = None) -> None:\n",
    "    \"\"\"\n",
    "    Stream comments.ndjson and build a hierarchical table with one row per submission\n",
    "    and *typed* list columns of comments (no cap). Writes with an explicit Arrow schema\n",
    "    using LARGE string/list types to avoid 2GB offset overflow.\n",
    "    \"\"\"\n",
    "    comments_by_sub: Dict[str, Dict[str, list]] = {}\n",
    "    sub_meta: Dict[str, Dict[str, object]] = {}\n",
    "\n",
    "    have_sub = sub_index_df.index\n",
    "    get_row = sub_index_df.loc\n",
    "\n",
    "    with comments_path.open('r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            if limit_lines and i > limit_lines:\n",
    "                break\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                c = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            sub_id = normalize_submission_id_from_link_id(c.get('link_id', ''))\n",
    "            if not sub_id or sub_id not in have_sub:\n",
    "                continue\n",
    "\n",
    "            if include_op_only and not c.get('is_submitter', False):\n",
    "                continue\n",
    "\n",
    "            if sub_id not in sub_meta:\n",
    "                s = get_row[sub_id]\n",
    "                sub_meta[sub_id] = {\n",
    "                    'submission_id': _to_str(sub_id),\n",
    "                    'submission_title': _to_str(s.get('title')),\n",
    "                    'submission_selftext': _to_str(s.get('selftext')),\n",
    "                    'submission_flair': _to_str(s.get('link_flair_text')),\n",
    "                    'submission_created_utc': _to_float(s.get('created_utc')),\n",
    "                }\n",
    "\n",
    "            if sub_id not in comments_by_sub:\n",
    "                comments_by_sub[sub_id] = {\n",
    "                    'comment_ids': [],\n",
    "                    'comment_bodies': [],\n",
    "                    'comment_scores': [],\n",
    "                    'comment_created_utcs': [],\n",
    "                    'is_submitters': []\n",
    "                }\n",
    "\n",
    "            buf = comments_by_sub[sub_id]\n",
    "            buf['comment_ids'].append(_to_str(c.get('id')))\n",
    "            buf['comment_bodies'].append(_to_str(c.get('body')))\n",
    "            buf['comment_scores'].append(_to_int(c.get('score')))\n",
    "            buf['comment_created_utcs'].append(_to_float(c.get('created_utc')))\n",
    "            buf['is_submitters'].append(_to_bool(c.get('is_submitter')))\n",
    "\n",
    "    # materialise to Arrow directly (skip pandas for writing nested columns)\n",
    "    rows = []\n",
    "    for sid, meta in sub_meta.items():\n",
    "        buf = comments_by_sub.get(sid, {\n",
    "            'comment_ids': [], 'comment_bodies': [], 'comment_scores': [],\n",
    "            'comment_created_utcs': [], 'is_submitters': []\n",
    "        })\n",
    "        rows.append({\n",
    "            **meta,\n",
    "            'comment_ids': buf['comment_ids'],\n",
    "            'comment_bodies': buf['comment_bodies'],\n",
    "            'comment_scores': buf['comment_scores'],\n",
    "            'comment_created_utcs': buf['comment_created_utcs'],\n",
    "            'is_submitters': buf['is_submitters'],\n",
    "            'n_comments': len(buf['comment_ids'])\n",
    "        })\n",
    "\n",
    "    # use LARGE string/list types to avoid offset overflow\n",
    "    STR = pa.large_string()\n",
    "    LSTR_LIST = pa.large_list(STR)\n",
    "\n",
    "    schema = pa.schema([\n",
    "        pa.field('submission_id', STR),\n",
    "        pa.field('submission_title', STR),\n",
    "        pa.field('submission_selftext', STR),\n",
    "        pa.field('submission_flair', STR),\n",
    "        pa.field('submission_created_utc', pa.float64()),\n",
    "        pa.field('comment_ids', LSTR_LIST),\n",
    "        pa.field('comment_bodies', LSTR_LIST),\n",
    "        pa.field('comment_scores', pa.list_(pa.int64())),\n",
    "        pa.field('comment_created_utcs', pa.list_(pa.float64())),\n",
    "        pa.field('is_submitters', pa.list_(pa.bool_())),\n",
    "        pa.field('n_comments', pa.int64()),\n",
    "    ])\n",
    "\n",
    "    def col(name, typ):\n",
    "        return pa.array([row.get(name) for row in rows], type=typ)\n",
    "\n",
    "    table = pa.table([\n",
    "        col('submission_id', STR),\n",
    "        col('submission_title', STR),\n",
    "        col('submission_selftext', STR),\n",
    "        col('submission_flair', STR),\n",
    "        col('submission_created_utc', pa.float64()),\n",
    "        col('comment_ids', LSTR_LIST),\n",
    "        col('comment_bodies', LSTR_LIST),\n",
    "        col('comment_scores', pa.list_(pa.int64())),\n",
    "        col('comment_created_utcs', pa.list_(pa.float64())),\n",
    "        col('is_submitters', pa.list_(pa.bool_())),\n",
    "        col('n_comments', pa.int64()),\n",
    "    ], schema=schema)\n",
    "\n",
    "    pq.write_table(table, out_path)\n",
    "    print(f'wrote hierarchical table (direct, typed): {out_path} ({table.num_rows} submissions)')\n",
    "\n",
    "# build-or-load guard (no need for an intermediate \"joined_*\" step)\n",
    "if remake_datafile or not hier_parquet.exists():\n",
    "    build_hierarchical_direct(\n",
    "        comments_path=comments_file,\n",
    "        sub_index_df=sub_df,          # from block 6 (indexed by 'id')\n",
    "        out_path=hier_parquet,\n",
    "        include_op_only=include_op_only,\n",
    "        limit_lines=None              # set e.g. 2_000 for a quick dry-run\n",
    "    )\n",
    "    # preview safely by reading only a projection (tiny) to avoid heavy nested ops\n",
    "    preview = pq.read_table(hier_parquet, columns=['submission_id', 'n_comments']).slice(0, 2).to_pandas()\n",
    "    display(preview)\n",
    "else:\n",
    "    hier_df = read_parquet_nested_safe(hier_parquet)\n",
    "    print(f'using existing hierarchical file: {hier_parquet} ({len(hier_df)} submissions)')\n",
    "    display(hier_df.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d4a85a",
   "metadata": {},
   "source": [
    "## Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02f4d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_spacy(nlp_name: str = 'en_core_web_sm'):\n",
    "    try:\n",
    "        return spacy.load(nlp_name, disable=['parser', 'textcat'])\n",
    "    except OSError as e:\n",
    "        raise RuntimeError(\n",
    "            f\"spaCy model {nlp_name!r} not installed. run: python -m spacy download {nlp_name}\"\n",
    "        ) from e\n",
    "\n",
    "nlp = ensure_spacy()\n",
    "\n",
    "url_re = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "nonword_re = re.compile(r\"[^a-zA-Z']+\")\n",
    "multispace_re = re.compile(r'\\s+')\n",
    "\n",
    "def clean_text(text: Optional[str]) -> str:\n",
    "    if not text:\n",
    "        return ''\n",
    "    text = url_re.sub(' ', text)\n",
    "    text = text.lower()\n",
    "    text = nonword_re.sub(' ', text)\n",
    "    text = multispace_re.sub(' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def lemmatize(text: str, nlp=nlp, do_ner: bool = True) -> Tuple[str, List[str]]:\n",
    "    if not text:\n",
    "        return '', []\n",
    "    doc = nlp(text)\n",
    "    lemmas = [t.lemma_ for t in doc if not (t.is_stop or t.is_punct or t.is_space)]\n",
    "    ents = [f'{ent.label_}:{ent.text}' for ent in doc.ents] if do_ner else []\n",
    "    return ' '.join(lemmas), ents\n",
    "\n",
    "def preprocess_submission_row(row: dict, do_ner: bool = True) -> dict:\n",
    "    raw = ' '.join([str(row.get('title') or ''), str(row.get('selftext') or '')]).strip()\n",
    "    cleaned = clean_text(raw)\n",
    "    lemmas, ents = lemmatize(cleaned, do_ner=do_ner)\n",
    "    return {\n",
    "        'id': row.get('id'),\n",
    "        'flair': row.get('link_flair_text'),\n",
    "        'created_utc': row.get('created_utc'),\n",
    "        'text_raw': raw,\n",
    "        'text_clean': cleaned,\n",
    "        'text_lemmas': lemmas,\n",
    "        'ents': ents\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17a996c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total submissions: 320671\n",
      "with usable body: 68011\n",
      "flair available: 79223\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_len</th>\n",
       "      <th>body_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>320671.000000</td>\n",
       "      <td>320671.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>61.227096</td>\n",
       "      <td>420.817483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>25.048349</td>\n",
       "      <td>883.220962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>44.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>57.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>74.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>311.000000</td>\n",
       "      <td>23331.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title_len       body_len\n",
       "count  320671.000000  320671.000000\n",
       "mean       61.227096     420.817483\n",
       "std        25.048349     883.220962\n",
       "min        20.000000       0.000000\n",
       "25%        44.000000       9.000000\n",
       "50%        57.000000       9.000000\n",
       "75%        74.000000       9.000000\n",
       "max       311.000000   23331.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usable submissions (submissions-only): 275324\n",
      "usable submissions (with comments): 274860 of 320065\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>n_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v2fdaf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v2fdq0</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  submission_id  n_comments\n",
       "1        v2fdaf           1\n",
       "2        v2fdq0          88"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# why: filter to usable posts once (before any NER) and cache them for fast reloads later.\n",
    "#      This creates:\n",
    "#        1) submissions_usable.parquet  (submissions only)\n",
    "#        2) submissions_with_comments_usable.parquet  (from hierarchical: title/body/comments)\n",
    "\n",
    "# thresholds\n",
    "min_body_len = 50       # chars required in submission selftext\n",
    "min_title_len = 40      # chars required in title\n",
    "min_comment_len = 150   # total chars across all comments (hierarchical path)\n",
    "\n",
    "removed_markers = {'[removed]', '[deleted]', None, ''}\n",
    "\n",
    "def is_removed(txt):\n",
    "    return (txt is None) or (str(txt).strip() in removed_markers)\n",
    "\n",
    "def text_len(s):\n",
    "    return 0 if s is None else len(str(s).strip())\n",
    "\n",
    "def total_comment_chars(bodies):\n",
    "    if not isinstance(bodies, list):\n",
    "        return 0\n",
    "    return sum(len(str(b or '')) for b in bodies)\n",
    "\n",
    "# ---------- submissions-only usable ----------\n",
    "# diagnostics on the full submissions index frame\n",
    "diag = pd.DataFrame({\n",
    "    'has_body': ~sub_df['selftext'].apply(is_removed),\n",
    "    'title_len': sub_df['title'].apply(text_len),\n",
    "    'body_len': sub_df['selftext'].apply(text_len),\n",
    "    'flair_none': sub_df['link_flair_text'].isna()\n",
    "})\n",
    "print('total submissions:', len(sub_df))\n",
    "print('with usable body:', int(diag['has_body'].sum()))\n",
    "print('flair available:', int((~diag[\"flair_none\"]).sum()))\n",
    "display(diag.describe())\n",
    "\n",
    "# keep if (body usable & body_len >= min_body_len) OR (title_len >= min_title_len)\n",
    "usable_mask = (\n",
    "    (~sub_df['selftext'].apply(is_removed) & (sub_df['selftext'].apply(text_len) >= min_body_len))\n",
    "    | (sub_df['title'].apply(text_len) >= min_title_len)\n",
    ")\n",
    "usable_sub_df = sub_df.loc[usable_mask, ['title','selftext','link_flair_text','created_utc']].copy()\n",
    "print('usable submissions (submissions-only):', len(usable_sub_df))\n",
    "\n",
    "# cache (submissions only)\n",
    "usable_submissions_parquet = interim_dir / 'submissions_usable.parquet'\n",
    "# uncomment if you want them saved:\n",
    "# usable_sub_df.to_parquet(usable_submissions_parquet, index=True)\n",
    "# print('saved:', usable_submissions_parquet)\n",
    "\n",
    "\n",
    "# ---------- hierarchical usable (submissions + comments) ----------\n",
    "# build from hierarchical parquet\n",
    "usable_hier_parquet = interim_dir / 'submissions_with_comments_usable.parquet'\n",
    "\n",
    "if 'hier_df' in globals() or (('hier_parquet' in globals()) and Path(hier_parquet).exists()):\n",
    "    # load hier_df if not already in memory\n",
    "    if 'hier_df' not in globals():\n",
    "        hier_df = pd.read_parquet(hier_parquet)\n",
    "\n",
    "    # usable if any of: selftext long enough, title long enough, OR enough total comment text\n",
    "    usable_hier_mask = (\n",
    "        ((~hier_df['submission_selftext'].apply(is_removed)) & (hier_df['submission_selftext'].apply(text_len) >= min_body_len))\n",
    "        | (hier_df['submission_title'].apply(text_len) >= min_title_len)\n",
    "        | (hier_df['comment_bodies'].apply(total_comment_chars) >= min_comment_len)\n",
    "    )\n",
    "    usable_hier_df = hier_df.loc[usable_hier_mask].copy()\n",
    "    print('usable submissions (with comments):', len(usable_hier_df), 'of', len(hier_df))\n",
    "\n",
    "    # save a compact projection that’s all we need for later preprocessing-from-hierarchical\n",
    "    cols_keep = [\n",
    "        'submission_id','submission_title','submission_selftext',\n",
    "        'submission_flair','submission_created_utc',\n",
    "        'comment_bodies','is_submitters','n_comments'\n",
    "    ]\n",
    "    # uncomment if you want them saved:\n",
    "    # usable_hier_df[cols_keep].to_parquet(usable_hier_parquet, index=False)\n",
    "    # print('saved:', usable_hier_parquet)\n",
    "\n",
    "    # quick peek\n",
    "    display(usable_hier_df.head(2)[['submission_id','n_comments']])\n",
    "else:\n",
    "    print('hierarchical parquet not found — skipping submissions+comments usable cache. '\n",
    "          'Build it first with build_hierarchical_direct().')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855144c",
   "metadata": {},
   "source": [
    "#### apply preproccessing to sumbitions and save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b555dcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: data\\parquet\\submissions_preprocessed.parquet (275324 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>flair</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>text_raw</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_lemmas</th>\n",
       "      <th>ents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v2fdaf</td>\n",
       "      <td>None</td>\n",
       "      <td>1654084958</td>\n",
       "      <td>AITA for uninviting a “best friend” to my gender reveal/housewarming party? [removed]</td>\n",
       "      <td>aita for uninviting a best friend to my gender reveal housewarming party removed</td>\n",
       "      <td>aita uninvite good friend gender reveal housewarme party remove</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v2fdq0</td>\n",
       "      <td>Not enough info</td>\n",
       "      <td>1654084986</td>\n",
       "      <td>AITA for being mad at my friends for not sticking to our agreements? [deleted]</td>\n",
       "      <td>aita for being mad at my friends for not sticking to our agreements deleted</td>\n",
       "      <td>aita mad friend stick agreement delete</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v2fgt3</td>\n",
       "      <td>None</td>\n",
       "      <td>1654085236</td>\n",
       "      <td>AITA for NEVER wanting to see a guy after he LIED about having children &amp;amp; his autism diagnosis? [removed]</td>\n",
       "      <td>aita for never wanting to see a guy after he lied about having children amp his autism diagnosis removed</td>\n",
       "      <td>aita want guy lie have child amp autism diagnosis remove</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id            flair  created_utc  \\\n",
       "0  v2fdaf             None   1654084958   \n",
       "1  v2fdq0  Not enough info   1654084986   \n",
       "2  v2fgt3             None   1654085236   \n",
       "\n",
       "                                                                                                        text_raw  \\\n",
       "0                          AITA for uninviting a “best friend” to my gender reveal/housewarming party? [removed]   \n",
       "1                                 AITA for being mad at my friends for not sticking to our agreements? [deleted]   \n",
       "2  AITA for NEVER wanting to see a guy after he LIED about having children &amp; his autism diagnosis? [removed]   \n",
       "\n",
       "                                                                                                 text_clean  \\\n",
       "0                          aita for uninviting a best friend to my gender reveal housewarming party removed   \n",
       "1                               aita for being mad at my friends for not sticking to our agreements deleted   \n",
       "2  aita for never wanting to see a guy after he lied about having children amp his autism diagnosis removed   \n",
       "\n",
       "                                                       text_lemmas ents  \n",
       "0  aita uninvite good friend gender reveal housewarme party remove   []  \n",
       "1                           aita mad friend stick agreement delete   []  \n",
       "2         aita want guy lie have child amp autism diagnosis remove   []  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# why: turn usable submissions into a clean, lemmatised, NER-tagged dataset and cache it for fast reloads\n",
    "\n",
    "submissions_preprocessed_parquet = interim_dir / 'submissions_preprocessed.parquet'\n",
    "build_submissions_preprocessed = True          # set False to skip rebuilding if file exists\n",
    "overwrite_submissions_preprocessed = False     # set True to force overwrite\n",
    "\n",
    "def preprocess_submissions_df(df_sub_indexed: pd.DataFrame, do_ner: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    df_sub_indexed: dataframe indexed by 'id' with columns ['title','selftext','link_flair_text','created_utc']\n",
    "    returns: dataframe with ['id','flair','created_utc','text_raw','text_clean','text_lemmas','ents']\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for sid, row in df_sub_indexed[['title','selftext','link_flair_text','created_utc']].iterrows():\n",
    "        row_adapted = {\n",
    "            'id': sid,\n",
    "            'title': row.get('title'),\n",
    "            'selftext': row.get('selftext'),\n",
    "            'link_flair_text': row.get('link_flair_text'),\n",
    "            'created_utc': row.get('created_utc'),\n",
    "        }\n",
    "        records.append(preprocess_submission_row(row_adapted, do_ner=do_ner))\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "if submissions_preprocessed_parquet.exists() and not overwrite_submissions_preprocessed and not build_submissions_preprocessed:\n",
    "    submissions_preprocessed_df = pd.read_parquet(submissions_preprocessed_parquet)\n",
    "    print('loaded:', submissions_preprocessed_parquet, f'({len(submissions_preprocessed_df)} rows)')\n",
    "else:\n",
    "    # source is the cached usable submissions from block 9 (indexed by 'id')\n",
    "    if 'usable_sub_df' not in globals():\n",
    "        raise RuntimeError('usable_sub_df not found. Run block 9 first to create/load it.')\n",
    "    submissions_preprocessed_df = preprocess_submissions_df(usable_sub_df, do_ner=True)\n",
    "    submissions_preprocessed_df.to_parquet(submissions_preprocessed_parquet, index=False)\n",
    "    print('saved:', submissions_preprocessed_parquet, f'({len(submissions_preprocessed_df)} rows)')\n",
    "\n",
    "display(submissions_preprocessed_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ad4fd",
   "metadata": {},
   "source": [
    "#### apply preproccessing to sumbitions+comments and save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ab190c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 1046966 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 138\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m     hier_source = hier_parquet\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m \u001b[43mpreprocess_hierarchical_in_chunks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhier_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhier_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparts_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mop_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_op_only_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# optional: merge all parts into a single file (can skip if you like dataset-of-parts)\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# merge_preprocessed_parts(parts_dir, out_merged_parquet)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mpreprocess_hierarchical_in_chunks\u001b[39m\u001b[34m(hier_path, out_dir, batch_rows, op_only, resume)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# clean first (cheap), then spaCy once via pipe\u001b[39;00m\n\u001b[32m     82\u001b[39m cleaned = [clean_text(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m all_raw]\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m docs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_process\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m lemmas = [\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(t.lemma_ \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m doc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (t.is_stop \u001b[38;5;129;01mor\u001b[39;00m t.is_punct \u001b[38;5;129;01mor\u001b[39;00m t.is_space)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[32m     85\u001b[39m ents = [[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.label_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m doc.ents] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rlvan\\Documents\\GitHub\\NLP_project\\.venv\\Lib\\site-packages\\spacy\\language.py:1622\u001b[39m, in \u001b[36mLanguage.pipe\u001b[39m\u001b[34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[39m\n\u001b[32m   1620\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[32m   1621\u001b[39m         docs = pipe(docs)\n\u001b[32m-> \u001b[39m\u001b[32m1622\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rlvan\\Documents\\GitHub\\NLP_project\\.venv\\Lib\\site-packages\\spacy\\util.py:1714\u001b[39m, in \u001b[36m_pipe\u001b[39m\u001b[34m(docs, proc, name, default_error_handler, kwargs)\u001b[39m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pipe\u001b[39m(\n\u001b[32m   1705\u001b[39m     docs: Iterable[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1706\u001b[39m     proc: \u001b[33m\"\u001b[39m\u001b[33mPipeCallable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1711\u001b[39m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m   1712\u001b[39m ) -> Iterator[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1713\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1714\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m proc.pipe(docs, **kwargs)\n\u001b[32m   1715\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1716\u001b[39m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[32m   1717\u001b[39m         kwargs = \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rlvan\\Documents\\GitHub\\NLP_project\\.venv\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:245\u001b[39m, in \u001b[36mpipe\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rlvan\\Documents\\GitHub\\NLP_project\\.venv\\Lib\\site-packages\\spacy\\util.py:1661\u001b[39m, in \u001b[36mminibatch\u001b[39m\u001b[34m(items, size)\u001b[39m\n\u001b[32m   1659\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   1660\u001b[39m     batch_size = \u001b[38;5;28mnext\u001b[39m(size_)\n\u001b[32m-> \u001b[39m\u001b[32m1661\u001b[39m     batch = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) == \u001b[32m0\u001b[39m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rlvan\\Documents\\GitHub\\NLP_project\\.venv\\Lib\\site-packages\\spacy\\util.py:1714\u001b[39m, in \u001b[36m_pipe\u001b[39m\u001b[34m(docs, proc, name, default_error_handler, kwargs)\u001b[39m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pipe\u001b[39m(\n\u001b[32m   1705\u001b[39m     docs: Iterable[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1706\u001b[39m     proc: \u001b[33m\"\u001b[39m\u001b[33mPipeCallable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1711\u001b[39m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m   1712\u001b[39m ) -> Iterator[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1713\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1714\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m proc.pipe(docs, **kwargs)\n\u001b[32m   1715\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1716\u001b[39m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[32m   1717\u001b[39m         kwargs = \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rlvan\\Documents\\GitHub\\NLP_project\\.venv\\Lib\\site-packages\\spacy\\pipeline\\pipe.pyx:48\u001b[39m, in \u001b[36mpipe\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rlvan\\Documents\\GitHub\\NLP_project\\.venv\\Lib\\site-packages\\spacy\\util.py:1714\u001b[39m, in \u001b[36m_pipe\u001b[39m\u001b[34m(docs, proc, name, default_error_handler, kwargs)\u001b[39m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pipe\u001b[39m(\n\u001b[32m   1705\u001b[39m     docs: Iterable[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1706\u001b[39m     proc: \u001b[33m\"\u001b[39m\u001b[33mPipeCallable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1711\u001b[39m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m   1712\u001b[39m ) -> Iterator[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1713\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1714\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m proc.pipe(docs, **kwargs)\n\u001b[32m   1715\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1716\u001b[39m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[32m   1717\u001b[39m         kwargs = \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rlvan\\Documents\\GitHub\\NLP_project\\.venv\\Lib\\site-packages\\spacy\\pipeline\\pipe.pyx:48\u001b[39m, in \u001b[36mpipe\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rlvan\\Documents\\GitHub\\NLP_project\\.venv\\Lib\\site-packages\\spacy\\util.py:1714\u001b[39m, in \u001b[36m_pipe\u001b[39m\u001b[34m(docs, proc, name, default_error_handler, kwargs)\u001b[39m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pipe\u001b[39m(\n\u001b[32m   1705\u001b[39m     docs: Iterable[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1706\u001b[39m     proc: \u001b[33m\"\u001b[39m\u001b[33mPipeCallable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1711\u001b[39m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m   1712\u001b[39m ) -> Iterator[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1713\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1714\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m proc.pipe(docs, **kwargs)\n\u001b[32m   1715\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1716\u001b[39m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[32m   1717\u001b[39m         kwargs = \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rlvan\\Documents\\GitHub\\NLP_project\\.venv\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:73\u001b[39m, in \u001b[36mpipe\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rlvan\\Documents\\GitHub\\NLP_project\\.venv\\Lib\\site-packages\\spacy\\util.py:1661\u001b[39m, in \u001b[36mminibatch\u001b[39m\u001b[34m(items, size)\u001b[39m\n\u001b[32m   1659\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   1660\u001b[39m     batch_size = \u001b[38;5;28mnext\u001b[39m(size_)\n\u001b[32m-> \u001b[39m\u001b[32m1661\u001b[39m     batch = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) == \u001b[32m0\u001b[39m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rlvan\\Documents\\GitHub\\NLP_project\\.venv\\Lib\\site-packages\\spacy\\util.py:1714\u001b[39m, in \u001b[36m_pipe\u001b[39m\u001b[34m(docs, proc, name, default_error_handler, kwargs)\u001b[39m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pipe\u001b[39m(\n\u001b[32m   1705\u001b[39m     docs: Iterable[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1706\u001b[39m     proc: \u001b[33m\"\u001b[39m\u001b[33mPipeCallable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1711\u001b[39m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m   1712\u001b[39m ) -> Iterator[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1713\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1714\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m proc.pipe(docs, **kwargs)\n\u001b[32m   1715\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1716\u001b[39m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[32m   1717\u001b[39m         kwargs = \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rlvan\\Documents\\GitHub\\NLP_project\\.venv\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:73\u001b[39m, in \u001b[36mpipe\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rlvan\\Documents\\GitHub\\NLP_project\\.venv\\Lib\\site-packages\\spacy\\util.py:1661\u001b[39m, in \u001b[36mminibatch\u001b[39m\u001b[34m(items, size)\u001b[39m\n\u001b[32m   1659\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   1660\u001b[39m     batch_size = \u001b[38;5;28mnext\u001b[39m(size_)\n\u001b[32m-> \u001b[39m\u001b[32m1661\u001b[39m     batch = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) == \u001b[32m0\u001b[39m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rlvan\\Documents\\GitHub\\NLP_project\\.venv\\Lib\\site-packages\\spacy\\language.py:1619\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1616\u001b[39m     docs = \u001b[38;5;28mself\u001b[39m._multiprocessing_pipe(texts, pipes, n_process, batch_size)\n\u001b[32m   1617\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1618\u001b[39m     \u001b[38;5;66;03m# if n_process == 1, no processes are forked.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1619\u001b[39m     docs = (\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts)\n\u001b[32m   1620\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[32m   1621\u001b[39m         docs = pipe(docs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rlvan\\Documents\\GitHub\\NLP_project\\.venv\\Lib\\site-packages\\spacy\\language.py:1132\u001b[39m, in \u001b[36mLanguage._ensure_doc\u001b[39m\u001b[34m(self, doc_like)\u001b[39m\n\u001b[32m   1130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[32m   1131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m.vocab).from_bytes(doc_like)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rlvan\\Documents\\GitHub\\NLP_project\\.venv\\Lib\\site-packages\\spacy\\language.py:1121\u001b[39m, in \u001b[36mLanguage.make_doc\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m   1115\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Turn a text into a Doc object.\u001b[39;00m\n\u001b[32m   1116\u001b[39m \n\u001b[32m   1117\u001b[39m \u001b[33;03mtext (str): The text to process.\u001b[39;00m\n\u001b[32m   1118\u001b[39m \u001b[33;03mRETURNS (Doc): The processed doc.\u001b[39;00m\n\u001b[32m   1119\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) > \u001b[38;5;28mself\u001b[39m.max_length:\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1122\u001b[39m         Errors.E088.format(length=\u001b[38;5;28mlen\u001b[39m(text), max_length=\u001b[38;5;28mself\u001b[39m.max_length)\n\u001b[32m   1123\u001b[39m     )\n\u001b[32m   1124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenizer(text)\n",
      "\u001b[31mValueError\u001b[39m: [E088] Text of length 1046966 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "# why: turn hierarchical rows (title + selftext + all comments) into cleaned/lemmatised/NER'd text,\n",
    "#      but do it in chunks to avoid RAM blowups and to allow resuming.\n",
    "\n",
    "import math\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "parts_dir = interim_dir / 'submissions_with_all_comments_preprocessed_parts'\n",
    "parts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_merged_parquet = interim_dir / 'submissions_with_all_comments_preprocessed.parquet'\n",
    "use_op_only_text = False        # True = only include OP comments in the text\n",
    "batch_rows = 1_000              # tune based on CPU/RAM\n",
    "resume = True                   # skip batches that already have a written part file\n",
    "\n",
    "def ensure_list_of_str(x):\n",
    "    # why: robustly coerce hierarchical column to a list[str]\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return [str(v) if v is not None else '' for v in x]\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return [str(v) if v is not None else '' for v in x.tolist()]\n",
    "    # sometimes a scalar sneaks in; treat as single-comment list\n",
    "    if isinstance(x, str):\n",
    "        return [x]\n",
    "    try:\n",
    "        return [str(v) for v in list(x)]\n",
    "    except Exception:\n",
    "        return [str(x)]\n",
    "\n",
    "def iter_hier_batches(path: Path, columns=None, batch_size: int = 1000):\n",
    "    \"\"\"\n",
    "    Stream the hierarchical parquet in record batches using ParquetFile.iter_batches,\n",
    "    yielding small pandas DataFrames. Works across PyArrow versions.\n",
    "    \"\"\"\n",
    "    pf = pq.ParquetFile(str(path))\n",
    "    # iter_batches returns pyarrow.RecordBatch objects in chunks\n",
    "    for batch in pf.iter_batches(batch_size=batch_size, columns=columns):\n",
    "        yield batch.to_pandas()\n",
    "\n",
    "\n",
    "def preprocess_hierarchical_in_chunks(hier_path: Path,\n",
    "                                      out_dir: Path,\n",
    "                                      batch_rows: int = 1000,\n",
    "                                      op_only: bool = False,\n",
    "                                      resume: bool = True):\n",
    "    cols = [\n",
    "        'submission_id','submission_title','submission_selftext',\n",
    "        'submission_flair','submission_created_utc',\n",
    "        'comment_bodies','is_submitters','n_comments'\n",
    "    ]\n",
    "    part_idx = 0\n",
    "    for pdf in iter_hier_batches(hier_path, columns=cols, batch_size=batch_rows):\n",
    "        part_path = out_dir / f'part-{part_idx:05d}.parquet'\n",
    "        if resume and part_path.exists():\n",
    "            part_idx += 1\n",
    "            continue\n",
    "\n",
    "        # build raw text (title + selftext + comments)\n",
    "        all_raw = []\n",
    "        n_used = []\n",
    "        for r in pdf.itertuples(index=False):\n",
    "            bodies = ensure_list_of_str(r.comment_bodies)\n",
    "            if op_only:\n",
    "                flags = ensure_list_of_str(r.is_submitters)  # may still be list/ndarray of bools; ensure_list gives strings\n",
    "                # convert flags back to bools where possible\n",
    "                flags_bool = []\n",
    "                for v in (r.is_submitters if isinstance(r.is_submitters, list) else (r.is_submitters.tolist() if isinstance(r.is_submitters, np.ndarray) else [])):\n",
    "                    flags_bool.append(bool(v))\n",
    "                if flags_bool:\n",
    "                    bodies = [b for b, f in zip(bodies, flags_bool) if f]\n",
    "            n_used.append(len(bodies))\n",
    "            raw = ' '.join([\n",
    "                str(r.submission_title or ''),\n",
    "                str(r.submission_selftext or ''),\n",
    "                ' '.join(bodies)\n",
    "            ]).strip()\n",
    "            all_raw.append(raw)\n",
    "\n",
    "        # clean first (cheap), then spaCy once via pipe\n",
    "        cleaned = [clean_text(t) for t in all_raw]\n",
    "        docs = list(nlp.pipe(cleaned, batch_size=64, n_process=1))\n",
    "        lemmas = [' '.join(t.lemma_ for t in doc if not (t.is_stop or t.is_punct or t.is_space)) for doc in docs]\n",
    "        ents = [[f'{e.label_}:{e.text}' for e in doc.ents] for doc in docs]\n",
    "\n",
    "        out_df = pd.DataFrame({\n",
    "            'id': pdf['submission_id'].astype(str).values,\n",
    "            'flair': pdf['submission_flair'].astype('string').where(pdf['submission_flair'].notna(), None),\n",
    "            'created_utc': pdf['submission_created_utc'].astype(float).values,\n",
    "            'text_raw': all_raw,\n",
    "            'text_clean': cleaned,\n",
    "            'text_lemmas': lemmas,\n",
    "            'ents': ents,\n",
    "            'n_comments': pdf['n_comments'].astype('Int64').values,\n",
    "            'n_comment_bodies_used': n_used\n",
    "        })\n",
    "\n",
    "        # write this chunk (use Arrow schema with large strings to be safe)\n",
    "        schema = pa.schema([\n",
    "            pa.field('id', pa.string()),\n",
    "            pa.field('flair', pa.string()),\n",
    "            pa.field('created_utc', pa.float64()),\n",
    "            pa.field('text_raw', pa.large_string()),\n",
    "            pa.field('text_clean', pa.large_string()),\n",
    "            pa.field('text_lemmas', pa.large_string()),\n",
    "            pa.field('ents', pa.list_(pa.string())),\n",
    "            pa.field('n_comments', pa.int64()),\n",
    "            pa.field('n_comment_bodies_used', pa.int64()),\n",
    "        ])\n",
    "        pq.write_table(pa.Table.from_pandas(out_df, schema=schema, preserve_index=False),\n",
    "                       part_path)\n",
    "        print(f'wrote {part_path} ({len(out_df)} rows)')\n",
    "        part_idx += 1\n",
    "\n",
    "    print('done preprocessing hierarchical in chunks.')\n",
    "\n",
    "def merge_preprocessed_parts(parts_dir: Path, out_path: Path):\n",
    "    # why: merge part files into a single parquet (optional; you can also keep the dir as a dataset)\n",
    "    parts = sorted(parts_dir.glob('part-*.parquet'))\n",
    "    if not parts:\n",
    "        print('no parts to merge.')\n",
    "        return\n",
    "    tables = [pq.read_table(p) for p in parts]\n",
    "    pq.write_table(pa.concat_tables(tables, promote=True), out_path)\n",
    "    print('merged', len(parts), 'parts into', out_path)\n",
    "\n",
    "# run it\n",
    "if 'usable_hier_df' in globals():\n",
    "    # if you built a usable subset parquet in block 9, use that file to stream\n",
    "    hier_source = interim_dir / 'submissions_with_comments_usable.parquet'\n",
    "    if not hier_source.exists():\n",
    "        # fallback to full hierarchical if usable subset file not written\n",
    "        hier_source = hier_parquet\n",
    "else:\n",
    "    hier_source = hier_parquet\n",
    "\n",
    "preprocess_hierarchical_in_chunks(\n",
    "    hier_path=hier_source,\n",
    "    out_dir=parts_dir,\n",
    "    batch_rows=batch_rows,\n",
    "    op_only=use_op_only_text,\n",
    "    resume=resume\n",
    ")\n",
    "\n",
    "# optional: merge all parts into a single file (can skip if you like dataset-of-parts)\n",
    "# merge_preprocessed_parts(parts_dir, out_merged_parquet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed542479",
   "metadata": {},
   "source": [
    "## Sample submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9898a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fetch_random_submissions_df_from_usable(usable_index_df: pd.DataFrame, n: int, seed: int = 42) -> pd.DataFrame:\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     ids = usable_index_df.index.values\n",
    "#     pick = ids if n >= len(ids) else rng.choice(ids, size=n, replace=False)\n",
    "#     df = usable_index_df.loc[pick, ['title', 'selftext', 'link_flair_text', 'created_utc']].reset_index()\n",
    "#     df = df.rename(columns={'index': 'id'})\n",
    "#     return df\n",
    "\n",
    "# def build_corpus(df_sub: pd.DataFrame, do_ner: bool = True) -> pd.DataFrame:\n",
    "#     processed = [preprocess_submission_row(row, do_ner=do_ner) for row in df_sub.to_dict('records')]\n",
    "#     return pd.DataFrame(processed)\n",
    "\n",
    "# df_sub_sample = fetch_random_submissions_df_from_usable(usable_sub_df, submission_sample_n, seed=random_seed)\n",
    "# print('submissions sampled for modelling (usable only):', len(df_sub_sample))\n",
    "# display(df_sub_sample.head(3))\n",
    "\n",
    "# df_corpus = build_corpus(df_sub_sample, do_ner=True)\n",
    "# display(df_corpus.head(3)[['id', 'flair', 'text_raw']])\n",
    "\n",
    "df_corpus = submissions_preprocessed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c01940",
   "metadata": {},
   "source": [
    "## LDA topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce45d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit LDA on lemmatised bag-of-words\n",
    "def fit_lda(texts: Iterable[str], max_features: int = 50_000, n_topics: int = 15, max_df: float = 0.5, min_df: int = 10, random_state: int = 42):\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_features=max_features,\n",
    "        stop_words='english',\n",
    "        max_df=max_df,\n",
    "        min_df=min_df\n",
    "    )\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    lda = LDA(n_components=n_topics, random_state=random_state, learning_method='batch')\n",
    "    W = lda.fit_transform(X)  # doc-topic matrix\n",
    "    H = lda.components_        # topic-term matrix\n",
    "    return lda, vectorizer, W, H\n",
    "\n",
    "lda, vect, W, H = fit_lda(df_corpus['text_lemmas'].tolist(), n_topics=15, random_state=random_seed)\n",
    "\n",
    "def top_words_per_topic(vect, H, topn: int = 15) -> List[List[str]]:\n",
    "    feature_names = np.array(vect.get_feature_names_out())\n",
    "    topics = []\n",
    "    for k in range(H.shape[0]):\n",
    "        idx = np.argsort(H[k])[::-1][:topn]\n",
    "        topics.append(feature_names[idx].tolist())\n",
    "    return topics\n",
    "\n",
    "topics_top_words = top_words_per_topic(vect, H, topn=15)\n",
    "for i, words in enumerate(topics_top_words):\n",
    "    print(f'topic {i:02d}:', ', '.join(words))\n",
    "\n",
    "# map topics to 5 categories using keyword overlap\n",
    "category_keywords = {\n",
    "    'finances': {\n",
    "        'money','pay','paid','rent','bill','bills','loan','debt','card','credit','cash','salary','bonus','split','cost','expensive','cheap','wedding','gift','refund','share','finance'\n",
    "    },\n",
    "    'relationship': {\n",
    "        'relationship','boyfriend','girlfriend','partner','date','dating','romantic','love','cheat','ex','fiancé','fiance','fiancee','breakup','trust','jealous'\n",
    "    },\n",
    "    'family_conflict': {\n",
    "        'mom','dad','mother','father','sister','brother','siblings','family','cousin','aunt','uncle','inlaws','in','law','grandma','grandpa','child','baby','pregnant','wedding','name'\n",
    "    },\n",
    "    'work': {\n",
    "        'work','job','boss','coworker','manager','shift','hours','office','remote','payroll','promotion','hr','fire','fired','leave','paternity','maternity'\n",
    "    },\n",
    "    'societal_norms': {\n",
    "        'culture','religion','religious','tradition','gender','pronoun','politics','law','legal','illegal','discrimination','racist','ableist','ethics','value','norm','boundary','consent'\n",
    "    }\n",
    "}\n",
    "\n",
    "def score_topic_to_category(words: List[str]) -> Tuple[str, Dict[str, int]]:\n",
    "    scores = {cat: 0 for cat in category_keywords}\n",
    "    wordset = set(words)\n",
    "    for cat, kw in category_keywords.items():\n",
    "        scores[cat] = len(wordset & kw)\n",
    "    best_cat = max(scores, key=scores.get)\n",
    "    return best_cat, scores\n",
    "\n",
    "topic_category = []\n",
    "for i, words in enumerate(topics_top_words):\n",
    "    best, scores = score_topic_to_category(words)\n",
    "    topic_category.append({'topic': i, 'category': best, **scores})\n",
    "\n",
    "df_topic_map = pd.DataFrame(topic_category).sort_values(['category', 'topic'])\n",
    "display(df_topic_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d67d9f",
   "metadata": {},
   "source": [
    "## Assign topic with NER bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52493cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = np.argmax(W, axis=1)\n",
    "df_corpus['topic'] = topic_labels\n",
    "\n",
    "topic_to_cat = {row['topic']: row['category'] for _, row in df_topic_map.iterrows()}\n",
    "df_corpus['category_initial'] = df_corpus['topic'].map(topic_to_cat).fillna('societal_norms')\n",
    "\n",
    "def ner_bias_category(ents: List[str], current: str) -> str:\n",
    "    labels = [e.split(':', 1)[0] for e in ents]\n",
    "    if any(lbl in ('NORP', 'LAW') for lbl in labels) and current in ('relationship', 'work', 'finances'):\n",
    "        return 'societal_norms'\n",
    "    if any(lbl in ('PERSON',) for lbl in labels) and current == 'societal_norms':\n",
    "        return 'family_conflict'\n",
    "    return current\n",
    "\n",
    "df_corpus['category'] = [\n",
    "    ner_bias_category(ents, cat) for ents, cat in zip(df_corpus['ents'], df_corpus['category_initial'])\n",
    "]\n",
    "\n",
    "category_counts = df_corpus['category'].value_counts().rename_axis('category').reset_index(name='count')\n",
    "display(category_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340ed459",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a6bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examples_by_category(df: pd.DataFrame, cat: str, k: int = 5) -> pd.DataFrame:\n",
    "    ex = df.loc[df['category'] == cat, ['id', 'flair', 'text_raw']].head(k).copy()\n",
    "    return ex\n",
    "\n",
    "for cat in ['finances', 'relationship', 'family_conflict', 'work', 'societal_norms']:\n",
    "    print(f'\\n=== {cat.upper()} EXAMPLES ===')\n",
    "    display(examples_by_category(df_corpus, cat, k=5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ti3160tu-nlp-group-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
